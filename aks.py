from html import parser
import os
import sys
import json
import argparse
import urllib.parse
import re
from openai import AzureOpenAI
from typing import List, Dict, Optional
import time
import base64
import requests
import hashlib
from ai_grader import AIResponseGrader, AKSResponseTester
from azure.ai.agents.models import BingGroundingTool

# Configuration
VECTOR_STORE_FILE = "vector_store_id.json"
ASSISTANT_ID_FILE = "assistant_id.json"
SUPPORTED_FORMATS = {".md", ".txt", ".json", ".yaml", ".yml"}
WIKI_URL_MAPPING_FILE = "wiki_url_mapping.json"
# Remove line 22 and replace lines 15-22 with:
from azure.ai.agents.models import BingGroundingTool

# Configuration
VECTOR_STORE_FILE = "vector_store_id.json"
ASSISTANT_ID_FILE = "assistant_id.json"
SUPPORTED_FORMATS = {".md", ".txt", ".json", ".yaml", ".yml"}
WIKI_URL_MAPPING_FILE = "wiki_url_mapping.json"

# Initialize Bing grounding tool only if connection name is available
# Initialize Bing grounding tool only if connection name is available
def get_bing_grounding_tool():
    print("üêõ DEBUG: Checking BING_CONNECTION_NAME...")
    # connection_name = os.getenv("BING_CONNECTION_NAME")
    connection_name = os.getenv("AZURE_BING_CONNECTION_ID")
    print(f"üêõ DEBUG: BING_CONNECTION_NAME = {connection_name}")
    
    if connection_name:
        print("üêõ DEBUG: Attempting to initialize BingGroundingTool...")
        try:
            tool = BingGroundingTool(connection_id=connection_name)
            print("üêõ DEBUG: ‚úÖ BingGroundingTool initialized successfully")
            return tool
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not initialize Bing grounding tool: {e}")
            print(f"üêõ DEBUG: Exception details: {type(e).__name__}: {str(e)}")
            return None
    else:
        print("üêõ DEBUG: No BING_CONNECTION_NAME found, skipping Bing tool")
        return None

class AKSWikiAssistant:
    def __init__(self):
        print("üêõ DEBUG: Starting AKSWikiAssistant initialization...")
        print(f"API Key set: {'Yes' if os.getenv('AZURE_OPENAI_API_KEY') else 'No'}")
        print(f"Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}")
        """Initialize the AKS Wiki Assistant with Azure OpenAI client"""
        
        print("üêõ DEBUG: Creating Azure OpenAI client...")
        self.client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version="2024-12-01-preview",
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        print("üêõ DEBUG: ‚úÖ Azure OpenAI client created")
        
        self.deployment_name = os.environ.get("AZURE_OPENAI_MODEL_EMAIL", "gpt-4.1")
        self.vector_store_id = None
        self.assistant_id = None
        self.thread_id = None
        
        print("üêõ DEBUG: Loading wiki URL mapping...")
        self.wiki_url_mapping = self.load_wiki_url_mapping()
        print("üêõ DEBUG: ‚úÖ Wiki URL mapping loaded")
        
        print("üêõ DEBUG: Initializing AI grader...")
        self.ai_grader = AIResponseGrader()
        print("üêõ DEBUG: ‚úÖ AI grader initialized")
        
        print("üêõ DEBUG: Initializing response tester...")
        self.response_tester = AKSResponseTester(self, self.ai_grader)
        print("üêõ DEBUG: ‚úÖ Response tester initialized")
        
        print("üêõ DEBUG: Initializing Bing tool...")
        self.bing_tool = get_bing_grounding_tool()
        print(f"üêõ DEBUG: ‚úÖ Bing tool result: {self.bing_tool is not None}")
        
        print("üêõ DEBUG: ‚úÖ AKSWikiAssistant initialization complete")


    def test_against_human_response(self, 
                                   question: str, 
                                   human_response: str,
                                   context: str = "",
                                   show_labels: bool = False) -> Dict:
        """
        Test AI response quality against a human response
        
        Args:
            question: The original question
            human_response: The human-generated response
            context: Additional context
            show_labels: Whether to reveal which response is AI vs human
            
        Returns:
            Dict containing evaluation results
        """
        return self.response_tester.test_response_quality(
            question=question,
            human_response=human_response,
            context=context,
            label_responses=show_labels
        )
    
    def run_evaluation_suite(self, test_cases_file: str = "test_cases.json") -> None:
        """
        Run evaluation suite from a test cases file
        
        Args:
            test_cases_file: Path to JSON file containing test cases
        """
        print(f"üß™ Running evaluation suite from {test_cases_file}...")
        
        try:
            with open(test_cases_file, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
        except FileNotFoundError:
            print(f"‚ùå Test cases file not found: {test_cases_file}")
            return
        except json.JSONDecodeError as e:
            print(f"‚ùå Error parsing test cases file: {e}")
            return
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nüìù Running test case {i}/{len(test_cases)}...")
            
            result = self.test_against_human_response(
                question=test_case["question"],
                human_response=test_case["human_response"],
                context=test_case.get("context", ""),
                show_labels=True
            )
            
            results.append(result)
            
            # Print summary for this test case
            if "evaluation" in result:
                winner = result.get("actual_winner", "Unknown")
                print(f"üèÜ Winner: {winner}")
                
                scores = result["evaluation"]["scores"]
                total_ai = scores["response_a"]["total"] if result["response_labels"]["response_a"] == "AI" else scores["response_b"]["total"]
                total_human = scores["response_b"]["total"] if result["response_labels"]["response_b"] == "Human" else scores["response_a"]["total"]
                
                print(f"üìä Scores - AI: {total_ai}/60, Human: {total_human}/60")
        
        # Generate final report
        print(f"\nüìã Generating final evaluation report...")
        
        # Count wins
        ai_wins = sum(1 for r in results if r.get("actual_winner") == "AI")
        human_wins = sum(1 for r in results if r.get("actual_winner") == "Human")
        
        print(f"\nüéØ Final Results:")
        print(f"   AI Wins: {ai_wins}")
        print(f"   Human Wins: {human_wins}")
        print(f"   AI Win Rate: {ai_wins/len(results)*100:.1f}%")
        
        # Save results
        results_file = f"evaluation_results_{int(time.time())}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Results saved to {results_file}")
    
    
    
    def load_wiki_url_mapping(self) -> Dict[str, str]:
        """Load the wiki URL mapping from file"""
        try:
            mapping_path = os.path.join(os.path.dirname(__file__), WIKI_URL_MAPPING_FILE)
            if os.path.exists(mapping_path):
                with open(mapping_path, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
                    print(f"‚úÖ Loaded {len(mapping)} wiki URL mappings")
                    return mapping
            else:
                print(f"‚ö†Ô∏è  Wiki URL mapping file not found at {mapping_path}")
                return {}
        except Exception as e:
            print(f"‚ùå Error loading wiki URL mapping: {e}")
            return {}

    def get_public_url(self, wiki_filename: str) -> Optional[str]:
        """Get the public URL for a wiki file using the mapping"""
        if not self.wiki_url_mapping:
            return None
        
        # Try exact match first
        if wiki_filename in self.wiki_url_mapping:
            mapping_value = self.wiki_url_mapping[wiki_filename]
            # Extract URL from markdown link format
            if mapping_value.startswith('[View this page online](') and mapping_value.endswith(')'):
                return mapping_value[24:-1]  # Remove '[View this page online](' and ')'
        
        # Try without extension
        filename_without_ext = wiki_filename.replace('.md', '')
        for key, value in self.wiki_url_mapping.items():
            if key.replace('.md', '') == filename_without_ext:
                if value.startswith('[View this page online](') and value.endswith(')'):
                    return value[24:-1]
        
        return None
    
    def process_wiki_files(self, wiki_path: str, subpath: str = "AKS", vector_store_id: str = None) -> List[str]:
        """Process cloned wiki files and optionally upload them incrementally"""
        print(f"\nüìÅ Processing wiki files from: {wiki_path}")
        
        # Look for AKS folder
        aks_path = os.path.join(wiki_path, subpath)
        if not os.path.exists(aks_path):
            print(f"‚ùå AKS folder not found at {aks_path}")
            return []
        
        processed_files = []
        base_wiki_url = "https://dev.azure.com/msazure/CloudNativeCompute/_wiki/wikis/CloudNativeCompute.wiki"
        
        # First, count total files
        total_files = sum(1 for root, dirs, files in os.walk(aks_path) 
                        for file in files if file.endswith('.md'))
        print(f"üìä Found {total_files} markdown files to process")
        
        file_count = 0
        batch_files = []
        batch_size = 50  # Upload every 50 files
        
        for root, dirs, files in os.walk(aks_path):
            for file in files:
                if file.endswith('.md'):
                    file_count += 1
                    file_path = os.path.join(root, file)
                    
                    # Calculate relative path for URL
                    relative_path = os.path.relpath(file_path, wiki_path)
                    wiki_path_for_url = "/" + relative_path.replace('.md', '').replace(os.sep, '/')
                    encoded_path = urllib.parse.quote(wiki_path_for_url)
                    wiki_url = f"{base_wiki_url}?pagePath={encoded_path}"
                    
                    # Read original content
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # Add wiki URL at the top if not already present
                        if not content.startswith("[View this page online]"):
                            modified_content = f"[View this page online]({wiki_url})\n\n{content}"
                            
                            # Write back the modified content
                            with open(file_path, 'w', encoding='utf-8') as f:
                                f.write(modified_content)
                        
                        processed_files.append(file_path)
                        
                        # Add to batch for incremental upload
                        if vector_store_id and any(file_path.lower().endswith(ext) for ext in SUPPORTED_FORMATS):
                            if os.path.getsize(file_path) > 0:
                                batch_files.append(file_path)
                        
                        # Show progress
                        if file_count % 100 == 0:
                            print(f"  ‚úì Processed {file_count}/{total_files} files...")
                        
                        # Upload batch if we hit the batch size
                        if vector_store_id and len(batch_files) >= batch_size:
                            self._upload_batch(vector_store_id, batch_files, file_count - len(batch_files) + 1, file_count)
                            batch_files = []
                        
                    except Exception as e:
                        print(f"  ‚ùå Error processing {file_path}: {e}")
        
        # Upload any remaining files
        if vector_store_id and batch_files:
            self._upload_batch(vector_store_id, batch_files, file_count - len(batch_files) + 1, file_count)
        
        print(f"‚úÖ Processed {len(processed_files)} wiki files total")
        return processed_files

    def _upload_batch(self, vector_store_id: str, file_paths: List[str], start_idx: int, end_idx: int) -> None:
        """Upload a batch of files to the vector store"""
        print(f"\n  üì§ Uploading files {start_idx} to {end_idx}...")
        
        file_streams = []
        for file_path in file_paths:
            try:
                file_streams.append(open(file_path, "rb"))
            except Exception as e:
                print(f"    ‚ùå Error opening {file_path}: {e}")
        
        if file_streams:
            try:
                file_batch = self.client.beta.vector_stores.file_batches.upload_and_poll(
                    vector_store_id=vector_store_id, 
                    files=file_streams
                )
                print(f"    ‚úÖ Upload complete - Status: {file_batch.status}")
                print(f"    üìä Completed: {file_batch.file_counts.completed}, Failed: {file_batch.file_counts.failed}")
                
                # Show sample of uploaded files
                if file_batch.file_counts.completed > 0:
                    print(f"    üìÅ Sample files uploaded:")
                    for i, path in enumerate(file_paths[:3]):  # Show first 3
                        print(f"       - {os.path.basename(path)}")
                    if len(file_paths) > 3:
                        print(f"       ... and {len(file_paths) - 3} more")
                        
            except Exception as e:
                print(f"    ‚ùå Error uploading batch: {e}")
            finally:
                for f in file_streams:
                    f.close()

    def create_or_load_vector_store(self, wiki_path: str, subpath: str = "AKS") -> str:
        """Create or load existing vector store"""
        print(f"\nüóÑÔ∏è  Setting up vector store...")
        
        # Check if vector store already exists
        if os.path.exists(VECTOR_STORE_FILE):
            with open(VECTOR_STORE_FILE, 'r') as f:
                data = json.load(f)
                self.vector_store_id = data.get('vector_store_id')
                print(f"‚úÖ Loaded existing vector store: {self.vector_store_id}")
                return self.vector_store_id
        
        # Process wiki files first
        processed_files = self.process_wiki_files(wiki_path, subpath, vector_store_id=None)

        if not processed_files:
            print("‚ùå No wiki files found to process.")
            return None

        # Test connection first
        print("üîå Testing Azure OpenAI connection...")
        try:
            test_response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10
            )
            print("‚úÖ Connection to Azure OpenAI successful")
        except Exception as e:
            print(f"‚ùå Connection failed: {e}")
            print(f"üí° Check your endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}")
            print(f"üí° It should look like: https://YOUR-RESOURCE-NAME.openai.azure.com/")
            return None

        # Create new vector store only after successful connection
        print("üì¶ Creating new vector store...")
        try:
            vector_store = self.client.beta.vector_stores.create(name="AKSWikiKnowledge")
            self.vector_store_id = vector_store.id
            
            # Save vector store ID immediately
            with open(VECTOR_STORE_FILE, "w") as f:
                json.dump({"vector_store_id": vector_store.id}, f)
            print(f"üíæ Vector store created: {self.vector_store_id}")
            
        except Exception as e:
            print(f"‚ùå Failed to create vector store: {e}")
            print("üí° Your Azure OpenAI deployment might not support the Assistants API")
            return None

        # Now process and upload files incrementally
        print("\nüöÄ Starting incremental file processing and upload...")
        processed_files = self.process_wiki_files(wiki_path, subpath, vector_store.id)

        if not processed_files:
            print("‚ùå No files were processed successfully")
            return None

        print(f"\n‚úÖ Vector store setup complete: {self.vector_store_id}")
        return self.vector_store_id

    def peek_vector_store(self) -> None:
        """Peek at files in the vector store"""
        if not self.vector_store_id:
            print("‚ùå No vector store loaded")
            return
        
        print(f"\nüëÄ Peeking into vector store: {self.vector_store_id}")
        
        # List files in the vector store
        files = self.client.beta.vector_stores.files.list(
            vector_store_id=self.vector_store_id,
            limit=10  # Just show first 10
        )
        
        print(f"\nüìÑ First few files in vector store:\n")
        for i, file in enumerate(files.data):
            print(f"{i+1}. File ID: {file.id}")
            
            # Retrieve file details
            file_details = self.client.files.retrieve(file.id)
            print(f"   Name: {file_details.filename}")
            print(f"   Size: {file_details.bytes} bytes")
            print(f"   Created: {file_details.created_at}")
            
            # Try to get content preview (if possible)
            try:
                content = self.client.files.content(file.id)
                preview = content.read().decode('utf-8')[:200] + "..."
                print(f"   Preview: {preview}\n")
            except:
                print(f"   Preview: Not available\n")

    def create_or_load_assistant(self) -> str:
        """Create or load existing assistant"""
        print(f"\nü§ñ Setting up AI assistant...")
        
        # Check if assistant already exists
        if os.path.exists(ASSISTANT_ID_FILE):
            with open(ASSISTANT_ID_FILE, 'r') as f:
                data = json.load(f)
                self.assistant_id = data.get('assistant_id')
                print(f"‚úÖ Loaded existing assistant: {self.assistant_id}")
                return self.assistant_id
        
        # Create new assistant
        print("üîß Creating new assistant...")
        try:
            assistant = self.client.beta.assistants.create(
                name="AKS Documentation Expert with Web Search",
                instructions="""You are an expert in AKS (Azure Kubernetes Service) documentation and support. 
                Use the provided documentation AND web search to answer technical questions accurately. 
                ALWAYS include links to the original documentation you referenced in your answer.
                When using web search, prioritize official Microsoft documentation and recent information.
                Be concise but thorough. Format your responses with proper markdown.""",
                model=self.deployment_name,
                tools=[{"type": "file_search"}],
            )
            
            # Update assistant with vector store
            assistant = self.client.beta.assistants.update(
                assistant_id=assistant.id,
                tool_resources={"file_search": {"vector_store_ids": [self.vector_store_id]}}
            )
            
            # Save assistant ID
            with open(ASSISTANT_ID_FILE, "w") as f:
                json.dump({"assistant_id": assistant.id}, f)
            
            self.assistant_id = assistant.id
            print(f"‚úÖ Assistant created: {self.assistant_id}")
            return self.assistant_id
            
        except Exception as e:
            print(f"‚ùå Failed to create assistant: {e}")
            return None

    # def ask_question(self, question: str) -> None:
    #     """Ask a question to the assistant"""
    #     if not self.thread_id:
    #         thread = self.client.beta.threads.create(
    #             tool_resources={
    #                 "file_search": {
    #                     "vector_store_ids": [self.vector_store_id]
    #                 }
    #             }
    #         )
    #         self.thread_id = thread.id
        
    #     # Add message to thread
    #     self.client.beta.threads.messages.create(
    #         thread_id=self.thread_id,
    #         role="user",
    #         content=question,  
    #     )
        
    #     # Run the assistant
    #     print("üîÑ Processing your question...")
    #     run = self.client.beta.threads.runs.create_and_poll(
    #         thread_id=self.thread_id,
    #         assistant_id=self.assistant_id,
    #         instructions="Search through the AKS documentation to answer this question. Cite specific documents you reference.",
    #         tools=[{"type": "file_search"}]
    #     )
        
    #     if run.status == 'completed':
    #         # Get messages
    #         messages = self.client.beta.threads.messages.list(
    #             thread_id=self.thread_id
    #         )
            
    #         # Get the latest assistant message
    #         for message in messages:
    #             if message.role == "assistant":
    #                 for content in message.content:
    #                     if hasattr(content, 'text'):
    #                         text_content = content.text.value
    #                         annotations = getattr(content.text, 'annotations', [])
                            
    #                          # Debug: Print what we're getting
    #                         print(f"DEBUG: Found {len(annotations)} annotations")
    #                         for ann in annotations:
    #                             print(f"DEBUG: Annotation type: {type(ann)}")
    #                             if hasattr(ann, 'file_citation'):
    #                                 print(f"DEBUG: File citation found!")
                
    #                         # Process citations
    #                         final_content = self.process_citations(text_content, annotations)
                            
    #                         print(f"\nü§ñ Assistant:\n{final_content}\n")
    #                         return
    #     else:
    #         print(f"‚ùå Run failed with status: {run.status}")

    def process_citations(self, message_content: str, annotations: List) -> str:
        """Process citations and add proper links using wiki URL mapping"""
        if not annotations:
            return message_content
        
        unique_citations = {}
        citations = []
        
        for index, annotation in enumerate(annotations):
            pattern = re.escape(annotation.text)
            message_content = re.sub(pattern, f" [{index}]", message_content)
            
            if hasattr(annotation, "file_citation"):
                file_citation = annotation.file_citation
                cited_file = self.client.files.retrieve(file_citation.file_id)
                
                # Extract filename
                file_name = cited_file.filename
                display_name = file_name.replace('.md', '')
                
                # Try to get public URL from mapping
                public_url = self.get_public_url(file_name)
                
                # Get quote preview
                quote = getattr(file_citation, "quote", "Document")
                preview_text = quote[:200] + "..." if len(quote) > 200 else quote
                
                # Only add unique citations
                citation_key = file_name  # Use filename as key for uniqueness
                if citation_key not in unique_citations:
                    unique_citations[citation_key] = len(citations)
                    
                    # Create citation with HTML links
                    if public_url:
                        citation_text = f'[{len(citations)}] <a href="{public_url}" target="_blank">{display_name}</a>'
                    else:
                        # Fallback to just the display name without URL
                        citation_text = f"[{len(citations)}] {display_name}"
                    
                    if quote != "Document" and quote:
                        citation_text += f"<br><blockquote>{preview_text}</blockquote>"
                    citations.append(citation_text)
                else:
                    # Replace with existing citation index
                    existing_index = unique_citations[citation_key]
                    message_content = message_content.replace(f"[{index}]", f"[{existing_index}]")
        
        # Add citations at the end with HTML formatting
        if citations:
            message_content += "<br><br><strong>Sources:</strong><br>" + "<br><br>".join(citations)
        
        return message_content

    def ask_question(self, question: str, return_response: bool = False, stream: bool = False):
        """Ask a question to the assistant"""
        print(f"üêõ DEBUG: ask_question called with: question='{question}', return_response={return_response}, stream={stream}")
        
        if not self.thread_id:
            print("üêõ DEBUG: Creating new thread...")
            thread = self.client.beta.threads.create(
                tool_resources={
                    "file_search": {
                        "vector_store_ids": [self.vector_store_id]
                    }
                }
            )
            self.thread_id = thread.id
            print(f"üêõ DEBUG: ‚úÖ Thread created: {self.thread_id}")
        else:
            print(f"üêõ DEBUG: Using existing thread: {self.thread_id}")
        
        # Add message to thread
        print("üêõ DEBUG: Adding message to thread...")
        self.client.beta.threads.messages.create(
            thread_id=self.thread_id,
            role="user",
            content=question,  
        )
        print("üêõ DEBUG: ‚úÖ Message added to thread")
        
        # Prepare tools list - use bing.definitions like the working wiki_assistant.py
        # Prepare tools list - extract the JSON-serializable format
        tools = [{"type": "file_search"}]
        # if self.bing_tool:
        #     try:
        #         # The BingGroundingTool.definitions returns a list with the tool definition
        #         # Extract the actual dict from the definitions
        #         if hasattr(self.bing_tool, 'definitions') and self.bing_tool.definitions:
        #             for tool_def in self.bing_tool.definitions:
        #                 # Convert to dict if it's not already
        #                 if hasattr(tool_def, '__dict__'):
        #                     # It's an object, need to convert to dict
        #                     tool_dict = {
        #                         "type": "bing_grounding",
        #                         "bing_grounding": {
        #                             "connection_id": os.getenv("AZURE_BING_CONNECTION_ID")
        #                         }
        #                     }
        #                     tools.append(tool_dict)
        #                 else:
        #                     # It's already a dict
        #                     tools.append(tool_def)
        #             print(f"üêõ DEBUG: Added Bing grounding tool")
        #         else:
        #             # Fallback to manual configuration
        #             tools.append({
        #                 "type": "bing_grounding",
        #                 "bing_grounding": {
        #                     "connection_id": os.getenv("AZURE_BING_CONNECTION_ID")
        #                 }
        #             })
        #             print("üêõ DEBUG: Added Bing grounding tool (fallback)")
        #     except Exception as e:
        #         print(f"üêõ DEBUG: Error adding Bing tools: {e}")
        #         print("üêõ DEBUG: Continuing without Bing grounding")
        # else:
        #     print("üêõ DEBUG: No Bing tool available")
        print(f"üêõ DEBUG: Final tools array: {tools}")

        # Run the assistant with explicit file search
        print("üîÑ Processing your question...")
        print("üêõ DEBUG: About to create assistant run...")
        
        if stream:
            print("üêõ DEBUG: Creating streaming run...")
            try:
                run = self.client.beta.threads.runs.create(
                    thread_id=self.thread_id,
                    assistant_id=self.assistant_id,
                    instructions="""You MUST search through the uploaded AKS documentation files to answer this question comprehensively. 

        SEARCH PRIORITY:
        1. Search the uploaded documentation files for official AKS guidance
        2. Use multiple search queries if needed to find comprehensive information
        3. Look for related topics and cross-references

        CITATION REQUIREMENTS:
        - ALWAYS cite specific documents you reference using the file_search tool
        - Include proper file names and relevant sections
        - Provide clear links to source documentation
        - If multiple sources cover the topic, synthesize the information

        FORMAT:
        - Clear answer with step-by-step guidance
        - Include relevant examples and best practices from the documentation""",
                    tools=tools,
                    stream=True
                )
                print("üêõ DEBUG: ‚úÖ Streaming run created successfully")
            except Exception as e:
                print(f"üêõ DEBUG: ‚ùå Error creating streaming run: {type(e).__name__}: {str(e)}")
                raise
            
            response_content = ""
            print("üêõ DEBUG: Starting to process stream events...")
            
            try:
                for event in run:
                    print(f"üêõ DEBUG: Stream event: {event.event}")
                    if event.event == 'thread.message.delta':
                        for content in event.data.delta.content:
                            if hasattr(content, 'text') and hasattr(content.text, 'value'):
                                chunk = content.text.value
                                response_content += chunk
                                yield chunk
                    elif event.event == 'thread.run.completed':
                        print("üêõ DEBUG: Run completed, processing citations...")
                        # Process final content with citations
                        messages = self.client.beta.threads.messages.list(thread_id=self.thread_id)
                        for message in messages:
                            if message.role == "assistant":
                                for content in message.content:
                                    if hasattr(content, 'text'):
                                        text_content = content.text.value
                                        annotations = getattr(content.text, 'annotations', [])
                                        final_content = self.process_citations(text_content, annotations)
                                        
                                        # Send any remaining content
                                        if len(final_content) > len(response_content):
                                            remaining = final_content[len(response_content):]
                                            yield remaining
                                        print("üêõ DEBUG: ‚úÖ Streaming response completed")
                                        return
            except Exception as e:
                print(f"üêõ DEBUG: ‚ùå Error processing stream: {type(e).__name__}: {str(e)}")
                raise
        else:
            print("üêõ DEBUG: Creating non-streaming run...")
            try:
                run = self.client.beta.threads.runs.create_and_poll(
                    thread_id=self.thread_id,
                    assistant_id=self.assistant_id,
                    instructions="""You MUST search through the uploaded AKS documentation files to answer this question comprehensively. 

        SEARCH PRIORITY:
        1. Search the uploaded documentation files for official AKS guidance
        2. Use multiple search queries if needed to find comprehensive information
        3. Look for related topics and cross-references

        CITATION REQUIREMENTS:
        - ALWAYS cite specific documents you reference using the file_search tool
        - Include proper file names and relevant sections
        - Provide clear links to source documentation
        - If multiple sources cover the topic, synthesize the information

        FORMAT:
        - Clear answer with step-by-step guidance
        - Use HTML links for citations: <a href="URL" target="_blank">Link Text</a>
        - Use basic HTML formatting: <strong>bold</strong>, <em>italic</em>, <br> for line breaks
        - Include relevant examples and best practices from the documentation
        - Do not use markdown - use HTML formatting only""",
                    tools=tools,
                )
                print(f"üêõ DEBUG: ‚úÖ Non-streaming run created with status: {run.status}")
            except Exception as e:
                print(f"üêõ DEBUG: ‚ùå Error creating non-streaming run: {type(e).__name__}: {str(e)}")
                raise
        
        if run.status == 'completed':
            print("üêõ DEBUG: Run completed successfully, processing messages...")
            # Get messages
            messages = self.client.beta.threads.messages.list(
                thread_id=self.thread_id
            )
            for message in messages:
                if message.role == "assistant":
                    for content in message.content:
                        if hasattr(content, 'text'):
                            text_content = content.text.value
                            annotations = getattr(content.text, 'annotations', [])
                            
                            # Process citations
                            final_content = self.process_citations(text_content, annotations)
                            
                            if return_response:
                                print("üêõ DEBUG: ‚úÖ Returning response")
                                return final_content
                            else:
                                print(f"\nü§ñ Assistant:\n{final_content}\n")
                                print("üêõ DEBUG: ‚úÖ Response printed")
                                return final_content
        else:
            print(f"‚ùå Run failed with status: {run.status}")
            print(f"üêõ DEBUG: ‚ùå Run failed with status: {run.status}")

    def interactive_mode(self) -> None:
        """Interactive question-answer mode"""
        print("\nüí¨ Interactive mode - Type 'exit' to quit\n")
        
        while True:
            question = input("You: ").strip()
            
            if question.lower() in ['exit', 'quit', 'q']:
                print("\nüëã Goodbye!")
                break
            
            if question:
                self.ask_question(question)

    def delete_vector_store(self) -> None:
        """Delete the vector store completely"""
        # Try to load from file if not already loaded
        if not self.vector_store_id and os.path.exists(VECTOR_STORE_FILE):
            with open(VECTOR_STORE_FILE, 'r') as f:
                data = json.load(f)
                self.vector_store_id = data.get('vector_store_id')
        
        if not self.vector_store_id:
            print("‚ùå No vector store found to delete")
            return
        
        print(f"\n‚ö†Ô∏è  WARNING: About to delete vector store: {self.vector_store_id}")
        confirm = input("Are you sure? This cannot be undone. Type 'yes' to confirm: ")
        
        if confirm.lower() != 'yes':
            print("‚ùå Deletion cancelled")
            return
        
        try:
            # Delete the vector store
            self.client.beta.vector_stores.delete(self.vector_store_id)
            print(f"‚úÖ Vector store {self.vector_store_id} deleted successfully")
            
            # Remove the saved file
            if os.path.exists(VECTOR_STORE_FILE):
                os.remove(VECTOR_STORE_FILE)
                print("‚úÖ Removed vector store ID file")
            
            # Also delete the assistant since it references this vector store
            if os.path.exists(ASSISTANT_ID_FILE):
                with open(ASSISTANT_ID_FILE, 'r') as f:
                    assistant_id = json.load(f).get('assistant_id')
                
                if assistant_id:
                    try:
                        self.client.beta.assistants.delete(assistant_id)
                        print(f"‚úÖ Deleted assistant {assistant_id}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Could not delete assistant: {e}")
                
                os.remove(ASSISTANT_ID_FILE)
                print("‚úÖ Removed assistant ID file")
            
            self.vector_store_id = None
            self.assistant_id = None
            
        except Exception as e:
            print(f"‚ùå Error deleting vector store: {e}")
    
    def download_ado_wiki(self, organization: str, project: str, wiki_name: str, 
                      pat: str, subpage_path: str, save_dir: str) -> None:
        """Download ADO wiki pages with proper hierarchy and progress tracking"""
        import base64
        import requests
        import time
        
        print(f"\nüì• Starting ADO Wiki Download")
        print(f"=" * 50)
        print(f"üìå Organization: {organization}")
        print(f"üìå Project: {project}")
        print(f"üìå Wiki: {wiki_name}")
        print(f"üìå Subpath: {subpage_path}")
        print(f"üìå Save to: {save_dir}")
        print(f"=" * 50)
        
        # Base URLs
        base_url = f"https://dev.azure.com/{organization}/{project}/_apis/wiki/wikis/{wiki_name}"
        web_url = f"https://dev.azure.com/{organization}/{project}/_wiki/wikis/{wiki_name}"
        auth = base64.b64encode(f":{pat}".encode()).decode()
        headers = {"Authorization": f"Basic {auth}"}

        os.makedirs(save_dir, exist_ok=True)
        
        # Fetch the full wiki page tree
        print("\nüîç Fetching wiki structure...")
        start_time = time.time()
        
        try:
            response = requests.get(
                f"{base_url}/pages?path=/&recursionLevel=Full&api-version=7.1",
                headers=headers
            )
            response.raise_for_status()
            data = response.json()
            print(f"‚úÖ Wiki structure fetched in {time.time() - start_time:.2f}s")
        except Exception as e:
            print(f"‚ùå Failed to fetch wiki structure: {e}")
            return

        # Handle different response formats
        if "value" in data:
            all_pages = data["value"]
        else:
            all_pages = self.flatten_pages(data)

        print(f"üìä Total pages in wiki: {len(all_pages)}")

        subpage_path = subpage_path.strip("/")
        filtered_pages = [
            p for p in all_pages
            if p.get("gitItemPath", "").startswith(f"/{subpage_path}")
        ]

        if not filtered_pages:
            print(f"‚ùå No pages found under '{subpage_path}'.")
            return

        print(f"üìÑ Found {len(filtered_pages)} pages under /{subpage_path}")
        
        # Track statistics
        success_count = 0
        error_count = 0
        total_size = 0
        error_pages = []
        
        # Process each filtered wiki page
        print(f"\n‚¨áÔ∏è  Downloading pages...")
        download_start = time.time()
        
        for i, page in enumerate(filtered_pages):
            path = page.get("path", "")
            
            # Show progress every 10 pages or for first/last 5
            if i < 5 or i >= len(filtered_pages) - 5 or (i + 1) % 10 == 0:
                print(f"\n[{i+1}/{len(filtered_pages)}] Processing: {path}")
            
            encoded_path = requests.utils.quote(path, safe="")

            try:
                # Get the content for this page
                content_response = requests.get(
                    f"{base_url}/pages?path={encoded_path}&includeContent=true&api-version=7.1-preview.1",
                    headers=headers
                )
                content_response.raise_for_status()
                
                content_data = content_response.json()
                content = content_data.get("content", "")
                
                # Show content size
                content_size = len(content.encode('utf-8'))
                total_size += content_size
                
                if i < 5 or i >= len(filtered_pages) - 5 or (i + 1) % 10 == 0:
                    print(f"    üìù Content size: {content_size:,} bytes")

                # Generate the live wiki URL for this page
                wiki_page_url = f"{web_url}/?pagePath={encoded_path}"
                modified_content = f"[View this page online]({wiki_page_url})\n\n{content}"

                # Create proper directory structure
                relative_path = path[len(f"/{subpage_path}"):].lstrip('/')
                
                if relative_path:
                    file_dir = os.path.join(save_dir, os.path.dirname(relative_path))
                    os.makedirs(file_dir, exist_ok=True)
                    filename = os.path.join(save_dir, f"{relative_path}.md")
                else:
                    filename = os.path.join(save_dir, f"{subpage_path}.md")

                with open(filename, "w", encoding="utf-8") as f:
                    f.write(modified_content)
                
                if i < 5 or i >= len(filtered_pages) - 5 or (i + 1) % 10 == 0:
                    print(f"    ‚úÖ Saved to: {os.path.relpath(filename, save_dir)}")
                
                success_count += 1
                
            except requests.exceptions.HTTPError as e:
                error_count += 1
                error_pages.append((path, str(e)))
                if i < 5 or i >= len(filtered_pages) - 5 or (i + 1) % 10 == 0:
                    print(f"    ‚ùå Error: {e}")
            except Exception as e:
                error_count += 1
                error_pages.append((path, str(e)))
                if i < 5 or i >= len(filtered_pages) - 5 or (i + 1) % 10 == 0:
                    print(f"    ‚ùå Unexpected error: {e}")
            
            # Show progress bar every 50 pages
            if (i + 1) % 50 == 0:
                progress = (i + 1) / len(filtered_pages) * 100
                elapsed = time.time() - download_start
                rate = (i + 1) / elapsed
                eta = (len(filtered_pages) - i - 1) / rate if rate > 0 else 0
                
                print(f"\nüìä Progress: {progress:.1f}% | "
                    f"Speed: {rate:.1f} pages/s | "
                    f"ETA: {eta:.0f}s | "
                    f"Success: {success_count} | "
                    f"Errors: {error_count}")

        # Final summary
        download_time = time.time() - download_start
        print(f"\n{'='*50}")
        print(f"‚úÖ Download Complete!")
        print(f"{'='*50}")
        print(f"üìä Summary:")
        print(f"   ‚Ä¢ Total pages processed: {len(filtered_pages)}")
        print(f"   ‚Ä¢ Successfully downloaded: {success_count}")
        print(f"   ‚Ä¢ Failed: {error_count}")
        print(f"   ‚Ä¢ Total size: {total_size / 1024 / 1024:.2f} MB")
        print(f"   ‚Ä¢ Time taken: {download_time:.1f}s")
        print(f"   ‚Ä¢ Average speed: {len(filtered_pages) / download_time:.1f} pages/s")
        print(f"   ‚Ä¢ Saved to: {os.path.abspath(save_dir)}")
        
        if error_pages:
            print(f"\n‚ö†Ô∏è  Failed pages:")
            for page, error in error_pages[:5]:  # Show first 5 errors
                print(f"   ‚Ä¢ {page}: {error}")
            if len(error_pages) > 5:
                print(f"   ... and {len(error_pages) - 5} more errors")
        
        # Check directory structure
        print(f"\nüìÅ Directory structure created:")
        for root, dirs, files in os.walk(save_dir):
            level = root.replace(save_dir, '').count(os.sep)
            indent = ' ' * 2 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 2 * (level + 1)
            
            # Show first few files in each directory
            for i, file in enumerate(files[:3]):
                print(f"{subindent}{file}")
            if len(files) > 3:
                print(f"{subindent}... and {len(files) - 3} more files")
            
            # Only show first 5 directories
            if level == 0 and len(dirs) > 5:
                for dir in dirs[:5]:
                    print(f"{indent}  {dir}/")
                print(f"{indent}  ... and {len(dirs) - 5} more directories")
                break

    def flatten_pages(self, page):
        """Helper for download_ado_wiki"""
        pages = []
        if "subPages" in page and page["subPages"]:
            for sub in page["subPages"]:
                pages.append(sub)
                pages.extend(self.flatten_pages(sub))
        return pages
    
    def download_ado_wiki_incremental(self, organization: str, project: str, wiki_name: str, 
                                 pat: str, subpage_path: str, save_dir: str) -> None:
        """Download ADO wiki pages incrementally, skipping unchanged files and updating changed ones"""
        # Track what's already downloaded with checksums
        downloaded_files_log = os.path.join(save_dir, "download_progress.json")
        downloaded_files = {}  # Changed to dict to store checksums
        
        # Load existing progress
        if os.path.exists(downloaded_files_log):
            with open(downloaded_files_log, 'r') as f:
                progress_data = json.load(f)
                downloaded_files = progress_data.get('downloaded_files', {})
                print(f"üìã Found existing progress: {len(downloaded_files)} files already tracked")
        
        print(f"\nüì• Starting Incremental ADO Wiki Download")
        print(f"=" * 50)
        print(f"üìå Organization: {organization}")
        print(f"üìå Project: {project}")
        print(f"üìå Wiki: {wiki_name}")
        print(f"üìå Subpath: {subpage_path}")
        print(f"üìå Save to: {save_dir}")
        print(f"=" * 50)
        
        # Base URLs
        base_url = f"https://dev.azure.com/{organization}/{project}/_apis/wiki/wikis/{wiki_name}"
        web_url = f"https://dev.azure.com/{organization}/{project}/_wiki/wikis/{wiki_name}"
        auth = base64.b64encode(f":{pat}".encode()).decode()
        headers = {"Authorization": f"Basic {auth}"}

        os.makedirs(save_dir, exist_ok=True)
        
        # Fetch the full wiki page tree
        print("\nüîç Fetching wiki structure...")
        start_time = time.time()
        
        try:
            response = requests.get(
                f"{base_url}/pages?path=/&recursionLevel=Full&api-version=7.1",
                headers=headers
            )
            response.raise_for_status()
            data = response.json()
            print(f"‚úÖ Wiki structure fetched in {time.time() - start_time:.2f}s")
        except Exception as e:
            print(f"‚ùå Failed to fetch wiki structure: {e}")
            return

        # Handle different response formats
        if "value" in data:
            all_pages = data["value"]
        else:
            all_pages = self.flatten_pages(data)

        print(f"üìä Total pages in wiki: {len(all_pages)}")

        subpage_path = subpage_path.strip("/")
        filtered_pages = [
            p for p in all_pages
            if p.get("gitItemPath", "").startswith(f"/{subpage_path}")
        ]

        if not filtered_pages:
            print(f"‚ùå No pages found under '{subpage_path}'.")
            return

        # Analyze what needs to be downloaded/updated
        pages_to_download = []
        pages_to_update = []
        unchanged_pages = []
        
        print(f"\nüîç Analyzing changes...")
        
        for i, page in enumerate(filtered_pages):
            path = page.get("path", "")
            
            # Show progress for analysis
            if (i + 1) % 1000 == 0:
                print(f"  Analyzed {i + 1}/{len(filtered_pages)} pages...")
            
            # Get page content to check if it's changed
            encoded_path = requests.utils.quote(path, safe="")
            try:
                content_response = requests.get(
                    f"{base_url}/pages?path={encoded_path}&includeContent=true&api-version=7.1-preview.1",
                    headers=headers
                )
                content_response.raise_for_status()
                content_data = content_response.json()
                content = content_data.get("content", "")
                
                # Calculate content hash
                content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
                
                # Check if we have this file and if it's changed
                if path not in downloaded_files:
                    pages_to_download.append((page, content, content_hash))
                elif downloaded_files[path] != content_hash:
                    pages_to_update.append((page, content, content_hash))
                else:
                    unchanged_pages.append(path)
                    
            except Exception as e:
                # If we can't get content, assume we need to download
                pages_to_download.append((page, None, None))
        
        print(f"\nüìä Analysis Results:")
        print(f"üìÑ Total pages under /{subpage_path}: {len(filtered_pages)}")
        print(f"‚úÖ Unchanged: {len(unchanged_pages)}")
        print(f"üÜï New to download: {len(pages_to_download)}")
        print(f"üîÑ Changed to update: {len(pages_to_update)}")
        
        all_pages_to_process = pages_to_download + pages_to_update
        
        if not all_pages_to_process:
            print("üéâ All pages are up to date!")
            return
        
        # Track statistics
        success_count = 0
        error_count = 0
        total_size = 0
        error_pages = []
        
        # Process pages that need downloading/updating
        print(f"\n‚¨áÔ∏è  Processing {len(all_pages_to_process)} pages...")
        download_start = time.time()
        
        for i, (page, content, content_hash) in enumerate(all_pages_to_process):
            path = page.get("path", "")
            is_update = path in downloaded_files
            
            # Show progress every 10 pages or for first/last 5
            if i < 5 or i >= len(all_pages_to_process) - 5 or (i + 1) % 10 == 0:
                action = "üîÑ Updating" if is_update else "üÜï Downloading"
                print(f"\n[{i+1}/{len(all_pages_to_process)}] {action}: {path}")
            
            try:
                # If we don't have content yet, fetch it
                if content is None:
                    encoded_path = requests.utils.quote(path, safe="")
                    content_response = requests.get(
                        f"{base_url}/pages?path={encoded_path}&includeContent=true&api-version=7.1-preview.1",
                        headers=headers
                    )
                    content_response.raise_for_status()
                    content_data = content_response.json()
                    content = content_data.get("content", "")
                    content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
                
                # Show content size
                content_size = len(content.encode('utf-8'))
                total_size += content_size
                
                if i < 5 or i >= len(all_pages_to_process) - 5 or (i + 1) % 10 == 0:
                    print(f"    üìù Content size: {content_size:,} bytes")

                # Generate the live wiki URL for this page
                encoded_path = requests.utils.quote(path, safe="")
                wiki_page_url = f"{web_url}/?pagePath={encoded_path}"

                # Resolve user GUIDs in the content if we have a PAT
                if pat:  # We have PAT available, so resolve GUIDs
                    content = self.resolve_ado_user_guids(content, organization, pat)

                modified_content = f"[View this page online]({wiki_page_url})\n\n{content}"

                # Create proper directory structure
                relative_path = path[len(f"/{subpage_path}"):].lstrip('/')
                
                if relative_path:
                    file_dir = os.path.join(save_dir, os.path.dirname(relative_path))
                    os.makedirs(file_dir, exist_ok=True)
                    filename = os.path.join(save_dir, f"{relative_path}.md")
                else:
                    filename = os.path.join(save_dir, f"{subpage_path}.md")

                with open(filename, "w", encoding="utf-8") as f:
                    f.write(modified_content)
                
                if i < 5 or i >= len(all_pages_to_process) - 5 or (i + 1) % 10 == 0:
                    print(f"    ‚úÖ Saved to: {os.path.relpath(filename, save_dir)}")
                
                success_count += 1
                downloaded_files[path] = content_hash
                
                # Save progress every 10 files
                if (i + 1) % 10 == 0:
                    with open(downloaded_files_log, 'w') as f:
                        json.dump({
                            'downloaded_files': downloaded_files,
                            'last_updated': time.time(),
                            'total_tracked': len(downloaded_files),
                            'last_session_stats': {
                                'new': len(pages_to_download),
                                'updated': len(pages_to_update),
                                'unchanged': len(unchanged_pages)
                            }
                        }, f)
                    
            except Exception as e:
                error_count += 1
                error_pages.append((path, str(e)))
                if i < 5 or i >= len(all_pages_to_process) - 5 or (i + 1) % 10 == 0:
                    print(f"    ‚ùå Error: {e}")
            
            # Show progress bar every 50 pages
            if (i + 1) % 50 == 0:
                progress = (i + 1) / len(all_pages_to_process) * 100
                elapsed = time.time() - download_start
                rate = (i + 1) / elapsed
                eta = (len(all_pages_to_process) - i - 1) / rate if rate > 0 else 0
                
                print(f"\nüìä Progress: {progress:.1f}% | "
                    f"Speed: {rate:.1f} pages/s | "
                    f"ETA: {eta:.0f}s | "
                    f"Success: {success_count} | "
                    f"Errors: {error_count}")

        # Save final progress
        with open(downloaded_files_log, 'w') as f:
            json.dump({
                'downloaded_files': downloaded_files,
                'last_updated': time.time(),
                'total_tracked': len(downloaded_files),
                'completed': True,
                'last_session_stats': {
                    'new': len(pages_to_download),
                    'updated': len(pages_to_update),
                    'unchanged': len(unchanged_pages)
                }
            }, f)

        # Final summary
        download_time = time.time() - download_start
        print(f"\n{'='*50}")
        print(f"‚úÖ Incremental Sync Complete!")
        print(f"{'='*50}")
        print(f"üìä Summary:")
        print(f"   ‚Ä¢ New pages downloaded: {len(pages_to_download)}")
        print(f"   ‚Ä¢ Pages updated: {len(pages_to_update)}")
        print(f"   ‚Ä¢ Unchanged pages: {len(unchanged_pages)}")
        print(f"   ‚Ä¢ Successfully processed: {success_count}")
        print(f"   ‚Ä¢ Failed: {error_count}")
        print(f"   ‚Ä¢ Total pages tracked: {len(downloaded_files)}")
        print(f"   ‚Ä¢ Total size this session: {total_size / 1024 / 1024:.2f} MB")
        print(f"   ‚Ä¢ Time taken: {download_time:.1f}s")
        print(f"   ‚Ä¢ Average speed: {len(all_pages_to_process) / download_time:.1f} pages/s" if download_time > 0 else "")
        print(f"   ‚Ä¢ Saved to: {os.path.abspath(save_dir)}")

    def check_download_status(self, save_dir: str) -> None:
        """Check what's been downloaded and what's missing"""
        downloaded_files_log = os.path.join(save_dir, "download_progress.json")
        
        if not os.path.exists(downloaded_files_log):
            print("‚ùå No download progress found. Run --download first.")
            return
        
        with open(downloaded_files_log, 'r') as f:
            progress_data = json.load(f)
        
        downloaded_files = progress_data.get('downloaded_files', {})
        last_updated = progress_data.get('last_updated', 0)
        completed = progress_data.get('completed', False)
        last_session = progress_data.get('last_session_stats', {})
        
        print(f"\nüìä Download Status Report")
        print(f"=" * 40)
        print(f"üìÅ Directory: {save_dir}")
        print(f"üìÑ Files tracked: {len(downloaded_files)}")
        print(f"üìÖ Last updated: {time.ctime(last_updated) if last_updated else 'Unknown'}")
        print(f"‚úÖ Completed: {'Yes' if completed else 'No'}")
        
        if last_session:
            print(f"\nüìà Last Session:")
            print(f"   üÜï New downloads: {last_session.get('new', 0)}")
            print(f"   üîÑ Updates: {last_session.get('updated', 0)}")
            print(f"   ‚úÖ Unchanged: {last_session.get('unchanged', 0)}")
        
        # Count actual files on disk
        actual_files = 0
        if os.path.exists(save_dir):
            for root, dirs, files in os.walk(save_dir):
                actual_files += len([f for f in files if f.endswith('.md')])
        
        print(f"üíæ Files on disk: {actual_files}")
        
        if actual_files != len(downloaded_files):
            print(f"‚ö†Ô∏è  Mismatch detected! Progress log shows {len(downloaded_files)} but found {actual_files} files")


    def rebuild_progress_from_existing_files(self, save_dir: str = "./downloaded_wiki/AKS") -> None:
        """Rebuild progress log by scanning existing downloaded files"""
        downloaded_files_log = os.path.join(save_dir, "download_progress.json")
        
        print(f"\nüîß Rebuilding progress log from existing files...")
        print(f"üìÅ Scanning directory: {save_dir}")
        
        if not os.path.exists(save_dir):
            print(f"‚ùå Directory {save_dir} doesn't exist")
            return
        
        # Scan all existing .md files
        existing_files = []
        for root, dirs, files in os.walk(save_dir):
            for file in files:
                if file.endswith('.md') and file != 'download_progress.json':
                    file_path = os.path.join(root, file)
                    existing_files.append(file_path)
        
        print(f"üìÑ Found {len(existing_files)} markdown files on disk")
        
        # Build progress tracking with dummy hashes (we'll calculate real ones on next download)
        downloaded_files = {}
        
        print(f"üîç Mapping files to ADO paths...")
        
        for i, file_path in enumerate(existing_files):
            # Show progress
            if (i + 1) % 1000 == 0 or i < 10:
                print(f"  Processing {i + 1}/{len(existing_files)}: {os.path.basename(file_path)}")
            
            try:
                # Convert file path back to ADO wiki path
                relative_path = os.path.relpath(file_path, save_dir)
                # Remove .md extension
                relative_path = relative_path[:-3] if relative_path.endswith('.md') else relative_path
                # Convert to ADO path format
                if relative_path == "AKS":
                    ado_path = "/AKS"
                else:
                    ado_path = f"/AKS/{relative_path}".replace(os.sep, '/')
                
                # Use dummy hash - will be updated on next download if content changed
                downloaded_files[ado_path] = "existing_file"
                
            except Exception as e:
                print(f"    ‚ùå Error processing {file_path}: {e}")
        
        # Save the rebuilt progress
        progress_data = {
            'downloaded_files': downloaded_files,
            'last_updated': time.time(),
            'total_tracked': len(downloaded_files),
            'completed': True,
            'rebuilt_from_existing': True,
            'rebuild_timestamp': time.time(),
            'files_found_on_disk': len(existing_files)
        }
        
        with open(downloaded_files_log, 'w') as f:
            json.dump(progress_data, f, indent=2)
        
        print(f"\n‚úÖ Progress log rebuilt successfully!")
        print(f"üìä Summary:")
        print(f"   ‚Ä¢ Files on disk: {len(existing_files)}")
        print(f"   ‚Ä¢ Files mapped to ADO paths: {len(downloaded_files)}")
        print(f"   ‚Ä¢ Progress log saved to: {downloaded_files_log}")
        print(f"\nüí° You can now run --download to sync any new/changed files")

    def resolve_ado_user_guids(self, content: str, organization: str, pat: str, cache: Dict[str, str] = None) -> str:
        """Resolve ADO user GUIDs in content to display names using the ADO API"""
        if cache is None:
            cache = {}
        
        # Pattern to match ADO user GUIDs: @<GUID>
        guid_pattern = r'@<([A-F0-9]{8}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{12})>'
        
        guids = re.findall(guid_pattern, content, re.IGNORECASE)
        if not guids:
            return content
        
        auth = base64.b64encode(f":{pat}".encode()).decode()
        headers = {"Authorization": f"Basic {auth}"}
        
        # Process each unique GUID
        for guid in set(guids):  # Use set to avoid duplicate API calls
            if guid in cache:
                # Use cached result
                display_name = cache[guid]
            else:
                # Look up the user via ADO API
                display_name = self._lookup_ado_user(organization, guid, headers)
                cache[guid] = display_name
            
            # Replace all occurrences of this GUID with the display name
            guid_pattern_specific = f'@<{guid}>'
            if display_name and display_name != 'Unknown User':
                replacement = f"@{display_name}"
            else:
                # Keep original if we couldn't resolve it
                replacement = f"@<{guid}>"
            
            content = content.replace(guid_pattern_specific, replacement)
        
        return content

    def _lookup_ado_user(self, organization: str, user_guid: str, headers: Dict[str, str]) -> str:
        """Look up a single user GUID and return their display name"""
        try:
            # Use the working vssps identities API
            url = f"https://vssps.dev.azure.com/{organization}/_apis/identities/{user_guid}?api-version=7.1"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                user_data = response.json()
                # Extract the display name from providerDisplayName field
                display_name = user_data.get('providerDisplayName', 'Unknown User')
                return display_name
            else:
                print(f"  ‚ö†Ô∏è  Could not resolve user {user_guid}: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Could not resolve user {user_guid}: {e}")
        
        return 'Unknown User'

    def resolve_guids_in_existing_files(self, organization: str, pat: str, wiki_dir: str) -> None:
        """Process existing downloaded wiki files to resolve user GUIDs"""
        print(f"\nüîÑ Resolving user GUIDs in existing files...")
        print(f"üìÇ Directory: {wiki_dir}")
        
        if not os.path.exists(wiki_dir):
            print(f"‚ùå Directory {wiki_dir} does not exist")
            return
        
        # Count total markdown files
        total_files = 0
        for root, dirs, files in os.walk(wiki_dir):
            total_files += sum(1 for file in files if file.endswith('.md'))
        
        print(f"üìä Found {total_files} markdown files to process")
        
        if total_files == 0:
            print("‚ÑπÔ∏è  No markdown files found")
            return
        
        # Shared cache for user GUID lookups
        user_cache = {}
        files_processed = 0
        files_modified = 0
        total_guids_resolved = 0
        
        for root, dirs, files in os.walk(wiki_dir):
            for file in files:
                if file.endswith('.md'):
                    files_processed += 1
                    file_path = os.path.join(root, file)
                    
                    try:
                        # Read the file
                        with open(file_path, 'r', encoding='utf-8') as f:
                            original_content = f.read()
                        
                        # Count GUIDs in this file
                        guid_pattern = r'@<([A-F0-9]{8}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{12})>'
                        guids_in_file = re.findall(guid_pattern, original_content, re.IGNORECASE)
                        
                        if guids_in_file:
                            # Resolve GUIDs in content
                            modified_content = self.resolve_ado_user_guids(
                                original_content, organization, pat, user_cache
                            )
                            
                            # Only write back if content actually changed
                            if modified_content != original_content:
                                with open(file_path, 'w', encoding='utf-8') as f:
                                    f.write(modified_content)
                                
                                files_modified += 1
                                unique_guids = len(set(guids_in_file))
                                total_guids_resolved += unique_guids
                                
                                relative_path = os.path.relpath(file_path, wiki_dir)
                                print(f"  ‚úÖ [{files_processed}/{total_files}] {relative_path} - resolved {unique_guids} user(s)")
                        
                        # Show progress for files without GUIDs too
                        elif files_processed % 50 == 0:
                            relative_path = os.path.relpath(file_path, wiki_dir)
                            print(f"  üìÑ [{files_processed}/{total_files}] {relative_path} - no GUIDs")
                            
                    except Exception as e:
                        relative_path = os.path.relpath(file_path, wiki_dir)
                        print(f"  ‚ùå Error processing {relative_path}: {e}")
        
        print(f"\n‚úÖ GUID Resolution Complete!")
        print(f"üìä Files processed: {files_processed}")
        print(f"üìù Files modified: {files_modified}")
        print(f"üë• Unique users resolved: {len(user_cache)}")
        print(f"üîó Total GUID references resolved: {total_guids_resolved}")
        
        if user_cache:
            print(f"\nüë• Resolved Users:")
            for guid, name in sorted(user_cache.items(), key=lambda x: x[1]):
                print(f"  {name} ({guid})")

    def create_test_vector_store(self, wiki_path: str, max_files: int = 10, subpath: str = "AKS") -> str:
        """Create a test vector store with just the first few files for testing"""
        print(f"\nüß™ Creating TEST vector store with max {max_files} files...")
        print(f"üìÅ Processing wiki files from: {wiki_path}")
        
        # Look for the downloaded wiki directory
        if not os.path.exists(wiki_path):
            print(f"‚ùå Wiki path not found at {wiki_path}")
            return None
        
        # Test connection first
        print("üîå Testing Azure OpenAI connection...")
        try:
            test_response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10
            )
            print("‚úÖ Connection to Azure OpenAI successful")
        except Exception as e:
            print(f"‚ùå Connection failed: {e}")
            return None

        # Create new vector store for testing
        print("üì¶ Creating TEST vector store...")
        try:
            vector_store = self.client.beta.vector_stores.create(name="AKSWikiKnowledge_TEST")
            self.vector_store_id = vector_store.id
            
            # Save vector store ID with test suffix
            test_vector_file = "test_vector_store_id.json"
            with open(test_vector_file, "w") as f:
                json.dump({"vector_store_id": vector_store.id}, f)
            print(f"üíæ TEST Vector store created: {self.vector_store_id}")
            
        except Exception as e:
            print(f"‚ùå Failed to create vector store: {e}")
            return None

        # Collect files to upload (preserving hierarchy)
        files_to_upload = []
        file_count = 0
        
        # Walk through the directory and collect files with their relative paths
        for root, dirs, files in os.walk(wiki_path):
            # Sort directories and files for consistent ordering
            dirs.sort()
            files.sort()
            
            for file in files:
                if file.endswith('.md') and file != 'download_progress.json':
                    if file_count >= max_files:
                        break
                        
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, wiki_path)
                    
                    files_to_upload.append({
                        'path': file_path,
                        'relative_path': relative_path,
                        'size': os.path.getsize(file_path)
                    })
                    file_count += 1
                    
            if file_count >= max_files:
                break
        
        print(f"\nüìä Selected {len(files_to_upload)} files for testing:")
        total_size = 0
        for i, file_info in enumerate(files_to_upload, 1):
            size_mb = file_info['size'] / 1024 / 1024
            total_size += file_info['size']
            print(f"  {i:2d}. {file_info['relative_path']} ({size_mb:.2f} MB)")
        
        print(f"\nüìà Total size: {total_size / 1024 / 1024:.2f} MB")
        
        # Confirm before uploading
        confirm = input(f"\n‚ùì Upload these {len(files_to_upload)} files to test vector store? (y/N): ")
        if confirm.lower() != 'y':
            print("‚ùå Upload cancelled")
            return None
        
        # Upload files in batches
        print(f"\nüì§ Uploading {len(files_to_upload)} files...")
        batch_size = 111  # Smaller batches for testing
        
        for i in range(0, len(files_to_upload), batch_size):
            batch = files_to_upload[i:i + batch_size]
            batch_files = [file_info['path'] for file_info in batch]
            
            print(f"\n  üì¶ Batch {i//batch_size + 1}: uploading {len(batch)} files...")
            for file_info in batch:
                print(f"    üìÑ {file_info['relative_path']}")
            
            self._upload_batch(vector_store.id, batch_files, i + 1, i + len(batch))
        
        print(f"\n‚úÖ TEST vector store setup complete!")
        print(f"üÜî Vector Store ID: {self.vector_store_id}")
        print(f"üíæ Saved to: test_vector_store_id.json")
        
        return self.vector_store_id

    def create_or_load_test_assistant(self) -> str:
        """Create or load test assistant"""
        print(f"\nü§ñ Setting up TEST AI assistant...")
        
        test_assistant_file = "test_assistant_id.json"
        
        # Check if test assistant already exists
        if os.path.exists(test_assistant_file):
            with open(test_assistant_file, 'r') as f:
                data = json.load(f)
                self.assistant_id = data.get('assistant_id')
                print(f"‚úÖ Loaded existing TEST assistant: {self.assistant_id}")
                return self.assistant_id
        
        # Create new test assistant
        print("üîß Creating new TEST assistant...")
        try:
            assistant = self.client.beta.assistants.create(
                name="AKS Documentation Expert (TEST)",
                instructions="""You are an expert in AKS (Azure Kubernetes Service) documentation. 
                This is a TEST environment with limited documentation.
                Use the provided documentation to answer technical questions accurately. 
                ALWAYS include the link to the original documentation you referenced in your answer.
                Be concise but thorough. Format your responses with proper markdown.
                Note: This is working with a limited test dataset.""",
                model=self.deployment_name,
                tools=[{"type": "file_search"}],
            )
            
            # Update assistant with vector store
            assistant = self.client.beta.assistants.update(
                assistant_id=assistant.id,
                tool_resources={"file_search": {"vector_store_ids": [self.vector_store_id]}}
            )
            
            # Save test assistant ID
            with open(test_assistant_file, "w") as f:
                json.dump({"assistant_id": assistant.id}, f)
            
            self.assistant_id = assistant.id
            print(f"‚úÖ TEST Assistant created: {self.assistant_id}")
            return self.assistant_id
            
        except Exception as e:
            print(f"‚ùå Failed to create TEST assistant: {e}")
            return None

    def cleanup_test_resources(self) -> None:
        """Clean up test vector store and assistant"""
        print(f"\nüßπ Cleaning up TEST resources...")
        
        # Load test resources
        test_vector_file = "test_vector_store_id.json"
        test_assistant_file = "test_assistant_id.json"
        
        # Delete test vector store
        if os.path.exists(test_vector_file):
            with open(test_vector_file, 'r') as f:
                data = json.load(f)
                vector_store_id = data.get('vector_store_id')
            
            if vector_store_id:
                try:
                    self.client.beta.vector_stores.delete(vector_store_id)
                    print(f"‚úÖ Deleted TEST vector store: {vector_store_id}")
                except Exception as e:
                    print(f"‚ùå Error deleting TEST vector store: {e}")
            
            os.remove(test_vector_file)
            print("‚úÖ Removed test vector store file")
        
        # Delete test assistant
        if os.path.exists(test_assistant_file):
            with open(test_assistant_file, 'r') as f:
                data = json.load(f)
                assistant_id = data.get('assistant_id')
            
            if assistant_id:
                try:
                    self.client.beta.assistants.delete(assistant_id)
                    print(f"‚úÖ Deleted TEST assistant: {assistant_id}")
                except Exception as e:
                    print(f"‚ùå Error deleting TEST assistant: {e}")
            
            os.remove(test_assistant_file)
            print("‚úÖ Removed test assistant file")
        
        print(f"üéâ TEST cleanup complete!")

    def peek_test_vector_store(self) -> None:
        """Peek at files in the test vector store"""
        test_vector_file = "test_vector_store_id.json"
        
        if not os.path.exists(test_vector_file):
            print("‚ùå No test vector store found. Run --test-setup first.")
            return
        
        with open(test_vector_file, 'r') as f:
            data = json.load(f)
            vector_store_id = data.get('vector_store_id')
        
        if not vector_store_id:
            print("‚ùå No test vector store ID found")
            return
        
        print(f"\nüëÄ Peeking into TEST vector store: {vector_store_id}")
        
        try:
            # List files in the vector store
            files = self.client.beta.vector_stores.files.list(
                vector_store_id=vector_store_id,
                limit=100  # Show more files for testing
            )
            
            print(f"\nüìÑ Files in test vector store ({len(files.data)} files):\n")
            for i, file in enumerate(files.data):
                print(f"{i+1}. File ID: {file.id}")
                
                # Retrieve file details
                file_details = self.client.files.retrieve(file.id)
                print(f"   Name: {file_details.filename}")
                print(f"   Size: {file_details.bytes} bytes")
                print(f"   Status: {getattr(file, 'status', 'unknown')}")
                
                # Try to get content preview for the first few files
                if i < 3:
                    try:
                        content = self.client.files.content(file.id)
                        preview = content.read().decode('utf-8')[:300] + "..."
                        print(f"   Preview: {preview}\n")
                    except Exception as e:
                        print(f"   Preview: Not available ({e})\n")
                else:
                    print()
                    
        except Exception as e:
            print(f"‚ùå Error peeking into vector store: {e}")

    def test_vector_store_search(self, query: str) -> None:
        """Test search functionality in the test vector store"""
        test_vector_file = "test_vector_store_id.json"
        test_assistant_file = "test_assistant_id.json"
        
        if not os.path.exists(test_vector_file) or not os.path.exists(test_assistant_file):
            print("‚ùå Test resources not found. Run --test-setup first.")
            return
        
        with open(test_vector_file, 'r') as f:
            vector_store_id = json.load(f).get('vector_store_id')
        
        with open(test_assistant_file, 'r') as f:
            assistant_id = json.load(f).get('assistant_id')
        
        print(f"\nüîç Testing search in vector store: {vector_store_id}")
        print(f"ü§ñ Using assistant: {assistant_id}")
        print(f"üîé Query: {query}")
        
        try:
            # Create a test thread
            thread = self.client.beta.threads.create(
                tool_resources={
                    "file_search": {
                        "vector_store_ids": [vector_store_id]
                    }
                }
            )
            
            # Add the query
            self.client.beta.threads.messages.create(
                thread_id=thread.id,
                role="user",
                content=f"Search for: {query}. Please show what documents you found and quote the exact text.",
            )
            
            # Run with detailed instructions
            run = self.client.beta.threads.runs.create_and_poll(
                thread_id=thread.id,
                assistant_id=assistant_id,
                instructions="""Search through the uploaded documents for the user's query. 
                ALWAYS cite the specific documents you found.
                Quote the exact text from the documents.
                If you can't find the information, say so explicitly.""",
                tools=[{"type": "file_search"}]
            )
            
            if run.status == 'completed':
                messages = self.client.beta.threads.messages.list(thread_id=thread.id)
                
                for message in messages:
                    if message.role == "assistant":
                        for content in message.content:
                            if hasattr(content, 'text'):
                                text_content = content.text.value
                                annotations = getattr(content.text, 'annotations', [])
                                
                                print(f"\nü§ñ Search Results:")
                                print(f"üìÑ Found {len(annotations)} citations")
                                
                                # Process and show citations
                                final_content = self.process_citations(text_content, annotations)
                                print(f"\n{final_content}")
                                return
            else:
                print(f"‚ùå Search failed with status: {run.status}")
                
        except Exception as e:
            print(f"‚ùå Error during search test: {e}")

    def count_test_vector_store_files(self) -> None:
        """Count the true total number of files in the test vector store"""
        test_vector_file = "test_vector_store_id.json"
        
        if not os.path.exists(test_vector_file):
            print("‚ùå No test vector store found. Run --test-setup first.")
            return
        
        with open(test_vector_file, 'r') as f:
            data = json.load(f)
            vector_store_id = data.get('vector_store_id')
        
        if not vector_store_id:
            print("‚ùå No test vector store ID found")
            return
        
        print(f"\nüî¢ Counting files in TEST vector store: {vector_store_id}")
        
        try:
            # Get all files by iterating through pages
            all_files = []
            after = None
            
            while True:
                # List files with pagination
                kwargs = {
                    'vector_store_id': vector_store_id,
                    'limit': 100
                }
                if after:
                    kwargs['after'] = after
                
                files_page = self.client.beta.vector_stores.files.list(**kwargs)
                all_files.extend(files_page.data)
                
                # Check if there are more files
                if not files_page.has_more:
                    break
                
                # Get the last file ID for pagination
                if files_page.data:
                    after = files_page.data[-1].id
                else:
                    break
            
            print(f"\nüìä Total files in test vector store: {len(all_files)}")
            
            # Show status breakdown
            status_counts = {}
            for file in all_files:
                status = getattr(file, 'status', 'unknown')
                status_counts[status] = status_counts.get(status, 0) + 1
            
            if status_counts:
                print(f"\nüìà Status breakdown:")
                for status, count in status_counts.items():
                    print(f"   {status}: {count} files")
                    
        except Exception as e:
            print(f"‚ùå Error counting files in vector store: {e}")

    # Add this new method to track uploaded files
    def create_incremental_vector_store(self, wiki_path: str, subpath: str = "AKS") -> str:
        """Create or load vector store with incremental file tracking"""
        print(f"\nüóÑÔ∏è  Setting up incremental vector store...")
        
        # Files to track what's been uploaded
        UPLOADED_FILES_LOG = "uploaded_files.json"
        uploaded_files = {}
        
        # Load existing vector store and uploaded files tracking
        if os.path.exists(VECTOR_STORE_FILE):
            with open(VECTOR_STORE_FILE, 'r') as f:
                data = json.load(f)
                self.vector_store_id = data.get('vector_store_id')
                
            if os.path.exists(UPLOADED_FILES_LOG):
                with open(UPLOADED_FILES_LOG, 'r') as f:
                    uploaded_files = json.load(f)
            
            print(f"‚úÖ Loaded existing vector store: {self.vector_store_id}")
            print(f"üìã Already uploaded: {len(uploaded_files)} files")
        else:
            # Create new vector store
            print("üîå Testing Azure OpenAI connection...")
            try:
                test_response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[{"role": "user", "content": "test"}],
                    max_tokens=10
                )
                print("‚úÖ Connection successful")
            except Exception as e:
                print(f"‚ùå Connection failed: {e}")
                return None

            print("üì¶ Creating new vector store...")
            try:
                vector_store = self.client.beta.vector_stores.create(name="AKSWikiKnowledge")
                self.vector_store_id = vector_store.id
                
                with open(VECTOR_STORE_FILE, "w") as f:
                    json.dump({"vector_store_id": vector_store.id}, f)
                print(f"üíæ Vector store created: {self.vector_store_id}")
            except Exception as e:
                print(f"‚ùå Failed to create vector store: {e}")
                return None

        # Process files incrementally
        print(f"\nüîç Scanning for new/changed files...")
        
        aks_path = os.path.join(wiki_path, subpath)
        if not os.path.exists(aks_path):
            print(f"‚ùå Path not found: {aks_path}")
            return None

        new_files = []
        updated_files = []
        unchanged_files = []
        
        for root, dirs, files in os.walk(aks_path):
            for file in files:
                if file.endswith('.md'):
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, aks_path)
                    
                    # Calculate file hash to detect changes
                    try:
                        with open(file_path, 'rb') as f:
                            file_hash = hashlib.md5(f.read()).hexdigest()
                        
                        if relative_path not in uploaded_files:
                            new_files.append((file_path, relative_path, file_hash))
                        elif uploaded_files[relative_path] != file_hash:
                            updated_files.append((file_path, relative_path, file_hash))
                        else:
                            unchanged_files.append(relative_path)
                            
                    except Exception as e:
                        print(f"‚ùå Error processing {file_path}: {e}")
        
        print(f"\nüìä File Analysis:")
        print(f"   üÜï New files: {len(new_files)}")
        print(f"   üîÑ Updated files: {len(updated_files)}")
        print(f"   ‚úÖ Unchanged files: {len(unchanged_files)}")
        
        files_to_upload = new_files + updated_files
        
        if not files_to_upload:
            print("üéâ All files are up to date!")
            return self.vector_store_id
        
        # Upload new/changed files
        print(f"\nüì§ Uploading {len(files_to_upload)} files...")
        
        batch_size = 50
        uploaded_count = 0
        
        for i in range(0, len(files_to_upload), batch_size):
            batch = files_to_upload[i:i + batch_size]
            batch_paths = [item[0] for item in batch]
            
            print(f"\n  üì¶ Batch {i//batch_size + 1}: {len(batch)} files")
            
            # Upload batch
            try:
                file_streams = [open(path, "rb") for path in batch_paths]
                
                file_batch = self.client.beta.vector_stores.file_batches.upload_and_poll(
                    vector_store_id=self.vector_store_id,
                    files=file_streams
                )
                
                print(f"    ‚úÖ Status: {file_batch.status}")
                print(f"    üìä Completed: {file_batch.file_counts.completed}")
                
                # Update tracking for successfully uploaded files
                if file_batch.status == 'completed':
                    for file_path, relative_path, file_hash in batch:
                        uploaded_files[relative_path] = file_hash
                        uploaded_count += 1
                
                # Close file streams
                for f in file_streams:
                    f.close()
                    
            except Exception as e:
                print(f"    ‚ùå Upload error: {e}")
        
        # Save updated tracking
        with open(UPLOADED_FILES_LOG, 'w') as f:
            json.dump(uploaded_files, f, indent=2)
        
        print(f"\n‚úÖ Incremental upload complete!")
        print(f"   üì§ Uploaded: {uploaded_count} files")
        print(f"   üìã Total tracked: {len(uploaded_files)} files")
        
        return self.vector_store_id

    def generate_response(self, question: str, context: str = ""):
        """Generate a streaming response to a question"""
        try:
            # Create a temporary thread for this question
            thread = self.client.beta.threads.create(
                tool_resources={
                    "file_search": {
                        "vector_store_ids": [self.vector_store_id]
                    }
                }
            )
            
            # Add the question with context
            full_question = f"{question}\n\nContext: {context}" if context else question
            
            self.client.beta.threads.messages.create(
                thread_id=thread.id,
                role="user",
                content=full_question
            )
            
            # Run the assistant with streaming
            run = self.client.beta.threads.runs.create(
                thread_id=thread.id,
                assistant_id=self.assistant_id,
                instructions="""You are an expert in AKS (Azure Kubernetes Service) support. 
                Provide a comprehensive, technically accurate response to this customer question.
                Use the documentation to provide specific guidance and actionable steps.
                Include relevant references and be professional in your tone.
                Format your response as plain text suitable for email, do not give any markdown formatting at all.""",
                tools=[{"type": "file_search"}],
                stream=True
            )
            
            response_content = ""
            
            for event in run:
                if event.event == 'thread.message.delta':
                    for content in event.data.delta.content:
                        if hasattr(content, 'text') and hasattr(content.text, 'value'):
                            chunk = content.text.value
                            response_content += chunk
                            yield chunk
                elif event.event == 'thread.run.completed':
                    # Process final content with citations
                    messages = self.client.beta.threads.messages.list(thread_id=thread.id)
                    for message in messages:
                        if message.role == "assistant":
                            for content in message.content:
                                if hasattr(content, 'text'):
                                    text_content = content.text.value
                                    annotations = getattr(content.text, 'annotations', [])
                                    final_content = self.process_citations(text_content, annotations)
                                    
                                    # Send any remaining content
                                    if len(final_content) > len(response_content):
                                        remaining = final_content[len(response_content):]
                                        yield remaining
                                    return
            
        except Exception as e:
            yield f"Error generating response: {str(e)}"
            
    def check_wiki_coverage(self) -> None:
        """Check how much of the eligible wiki content hasn't been scraped"""
        print(f"\nüîç Checking wiki coverage...")
        
        # Count downloaded files
        downloaded_dir = "./downloaded_wiki/AKS"
        downloaded_count = 0
        downloaded_size = 0
        
        if os.path.exists(downloaded_dir):
            for root, dirs, files in os.walk(downloaded_dir):
                for file in files:
                    if file.endswith('.md'):
                        file_path = os.path.join(root, file)
                        downloaded_count += 1
                        downloaded_size += os.path.getsize(file_path)
        
        print(f"üìÅ Downloaded files: {downloaded_count}")
        print(f"üíæ Downloaded size: {downloaded_size / 1024 / 1024:.2f} MB")
        
        # Check vector store files
        uploaded_files_log = "uploaded_files.json"
        uploaded_count = 0
        
        if os.path.exists(uploaded_files_log):
            with open(uploaded_files_log, 'r') as f:
                uploaded_files = json.load(f)
                uploaded_count = len(uploaded_files)
        
        print(f"‚òÅÔ∏è  Uploaded to vector store: {uploaded_count}")
        
        # Estimate remaining from ADO (this would require API call)
        print(f"\nüí° To check total available pages, run:")
        print(f"   python3 aks.py --check-ado-total")
        
def main():
    parser = argparse.ArgumentParser(description="AKS Wiki Assistant CLI")
    parser.add_argument("--setup", action="store_true", help="Setup vector store and assistant")
    parser.add_argument("--ask", type=str, help="Ask a single question")
    parser.add_argument("--interactive", action="store_true", help="Interactive Q&A mode")
    parser.add_argument("--wiki-path", type=str, default="./CloudNativeCompute.wiki", 
                       help="Path to cloned wiki repository")
    parser.add_argument("--subpath", type=str, default="AKS", 
                       help="Subpath within wiki to process (default: AKS)")
    parser.add_argument("--peek", action="store_true", help="Peek at vector store contents")
    parser.add_argument("--delete", action="store_true", help="Delete vector store and assistant")
    parser.add_argument("--download", action="store_true", help="Download wiki from ADO")
    # Add a new command to check download status
    parser.add_argument("--check-download", action="store_true", help="Check download progress")
    parser.add_argument("--rebuild-progress", action="store_true", help="Rebuild progress log from existing files")
    parser.add_argument("--resolve-guids", action="store_true", help="Resolve user GUIDs in existing downloaded files")
    # New test commands
    parser.add_argument("--test-setup", action="store_true", help="Setup TEST vector store with limited files")
    parser.add_argument("--test-files", type=int, default=10, help="Number of files for test setup (default: 10)")
    parser.add_argument("--test-ask", type=str, help="Ask a question using TEST assistant")
    parser.add_argument("--test-interactive", action="store_true", help="Interactive mode with TEST assistant")
    parser.add_argument("--cleanup-test", action="store_true", help="Clean up TEST resources")
    parser.add_argument("--downloaded-wiki-path", type=str, default="./downloaded_wiki/AKS", 
                       help="Path to downloaded wiki files (default: ./downloaded_wiki/AKS)")
    
    parser.add_argument("--peek-test", action="store_true", help="Peek at test vector store contents")
    parser.add_argument("--test-search", type=str, help="Test search functionality in test vector store")
    # Add to parser arguments:
    parser.add_argument("--check-coverage", action="store_true", help="Check wiki coverage and upload status")
    parser.add_argument("--incremental-setup", action="store_true", help="Setup vector store with incremental upload tracking")
    parser.add_argument("--count-test-files", action="store_true", help="Count total files in test vector store")
    parser.add_argument("--test-response", action="store_true", help="Test AI response against human response")
    parser.add_argument("--question", type=str, help="Question to test")
    parser.add_argument("--human-response", type=str, help="Human response to compare against")
    parser.add_argument("--context", type=str, default="", help="Additional context for the question")
    parser.add_argument("--show-labels", action="store_true", help="Show which response is AI vs human")
    parser.add_argument("--run-evaluation-suite", action="store_true", help="Run full evaluation suite")
    parser.add_argument("--test-cases-file", type=str, default="test_cases.json", help="Path to test cases JSON file")

    args = parser.parse_args()
    
    # Check environment variables
    if not os.getenv("AZURE_OPENAI_API_KEY") or not os.getenv("AZURE_OPENAI_ENDPOINT"):
        print("‚ùå Please set AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT environment variables")
        sys.exit(1)
    
    assistant = AKSWikiAssistant()
    

    if args.test_response:
        if not args.question or not args.human_response:
            print("‚ùå Please provide both --question and --human-response for testing")
            sys.exit(1)
        
        # Load assistant
        if os.path.exists(VECTOR_STORE_FILE):
            with open(VECTOR_STORE_FILE, 'r') as f:
                data = json.load(f)
                assistant.vector_store_id = data.get('vector_store_id')
        
        if os.path.exists(ASSISTANT_ID_FILE):
            with open(ASSISTANT_ID_FILE, 'r') as f:
                data = json.load(f)
                assistant.assistant_id = data.get('assistant_id')
        
        if not assistant.vector_store_id or not assistant.assistant_id:
            print("‚ùå Please set up the assistant first with --setup")
            sys.exit(1)
        
        # Run single test
        result = assistant.test_against_human_response(
            question=args.question,
            human_response=args.human_response,
            context=args.context,
            show_labels=args.show_labels
        )
        
        # Print results
        assistant.ai_grader.print_evaluation_summary(result)
        return
    
    if args.run_evaluation_suite:
        # Load assistant
        if os.path.exists(VECTOR_STORE_FILE):
            with open(VECTOR_STORE_FILE, 'r') as f:
                data = json.load(f)
                assistant.vector_store_id = data.get('vector_store_id')
        
        if os.path.exists(ASSISTANT_ID_FILE):
            with open(ASSISTANT_ID_FILE, 'r') as f:
                data = json.load(f)
                assistant.assistant_id = data.get('assistant_id')
        
        if not assistant.vector_store_id or not assistant.assistant_id:
            print("‚ùå Please set up the assistant first with --setup")
            sys.exit(1)
        
        assistant.run_evaluation_suite(args.test_cases_file)
        return
    
    # Handle debug commands
    if args.peek_test:
        assistant.peek_test_vector_store()
        return
    
    if args.test_search:
        assistant.test_vector_store_search(args.test_search)
        return
    
    # In main(), replace the setup section with:
    if args.setup or args.ask or args.interactive:
        # Comment out the wiki processing check
        # if not os.path.exists(args.wiki_path):
        #     print(f"‚ùå Wiki path '{args.wiki_path}' not found.")
        #     print(f"üí° Make sure you've downloaded the wiki first using --download")
        #     sys.exit(1)
        
        # Load vector store from file
        if os.path.exists(VECTOR_STORE_FILE):
            with open(VECTOR_STORE_FILE, 'r') as f:
                data = json.load(f)
                assistant.vector_store_id = data['vector_store_id']
                print(f"‚úÖ Using existing vector store: {assistant.vector_store_id}")
        else:
            print("‚ùå No vector store found. Run --setup first.")
            sys.exit(1)
            
        assistant.create_or_load_assistant()
        if not assistant.assistant_id:
            print("‚ùå Failed to create assistant")
            sys.exit(1)
    
    # Handle test questions
    if args.test_ask or args.test_interactive:
        # Load test assistant
        test_vector_file = "test_vector_store_id.json"
        test_assistant_file = "test_assistant_id.json"
        
        if not os.path.exists(test_vector_file) or not os.path.exists(test_assistant_file):
            print("‚ùå Test resources not found. Run --test-setup first.")
            sys.exit(1)
        
        with open(test_vector_file, 'r') as f:
            assistant.vector_store_id = json.load(f).get('vector_store_id')
        
        with open(test_assistant_file, 'r') as f:
            assistant.assistant_id = json.load(f).get('assistant_id')
        
        if args.test_ask:
            print("üß™ Using TEST assistant...")
            assistant.ask_question(args.test_ask)
        elif args.test_interactive:
            print("üß™ Using TEST assistant in interactive mode...")
            assistant.interactive_mode()
        return
    
    # Add to command handling:
    if args.check_coverage:
        assistant.check_wiki_coverage()
        return

    if args.incremental_setup:
        assistant.create_incremental_vector_store(args.downloaded_wiki_path)
        if assistant.vector_store_id:
            assistant.create_or_load_assistant()
        return
    
    # Handle test cleanup
    if args.cleanup_test:
        assistant.cleanup_test_resources()
        return
    
    # Handle test setup
    if args.test_setup:
        if not os.path.exists(args.downloaded_wiki_path):
            print(f"‚ùå Downloaded wiki path '{args.downloaded_wiki_path}' not found.")
            print(f"üí° Make sure you've downloaded the wiki first using --download")
            sys.exit(1)
        
        assistant.create_test_vector_store(args.downloaded_wiki_path, args.test_files)
        if not assistant.vector_store_id:
            print("‚ùå Failed to create test vector store")
            sys.exit(1)
            
        assistant.create_or_load_test_assistant()
        if not assistant.assistant_id:
            print("‚ùå Failed to create test assistant")
            sys.exit(1)
        return
    
    # For setup, we need the wiki path
    if args.setup:
        if not os.path.exists(args.wiki_path):
            print(f"‚ùå Wiki path '{args.wiki_path}' not found.")
            print(f"üí° Make sure you've cloned the wiki repository to this location.")
            sys.exit(1)
        
        assistant.create_or_load_vector_store(args.wiki_path, args.subpath)
        if not assistant.vector_store_id:
            print("‚ùå Failed to create vector store")
            sys.exit(1)
            
        assistant.create_or_load_assistant()
        if not assistant.assistant_id:
            print("‚ùå Failed to create assistant")
            sys.exit(1)
    
    
    # In the args handling section:
    if args.delete:
        assistant.delete_vector_store()
        return
    
    if args.count_test_files:
        assistant.count_test_vector_store_files()
        return
    
    # In main():
    if args.download:
        assistant.download_ado_wiki_incremental(  # Changed method name
            organization="msazure",
            project="CloudNativeCompute",
            wiki_name="CloudNativeCompute.wiki",
            pat=os.getenv("AZURE_DEVOPS_PAT", "your_pat_here"),
            subpage_path="/AKS",
            save_dir="./downloaded_wiki/AKS"
        )
        return
    
    if args.resolve_guids:
        assistant.resolve_guids_in_existing_files(
            organization="msazure",
            pat=os.getenv("AZURE_DEVOPS_PAT", "your_pat_here"),
            wiki_dir="./downloaded_wiki/AKS"
        )
        return

    if args.peek:
        # Just load the existing vector store ID
        if os.path.exists(VECTOR_STORE_FILE):
            with open(VECTOR_STORE_FILE, 'r') as f:
                data = json.load(f)
                assistant.vector_store_id = data.get('vector_store_id')
                assistant.peek_vector_store()
        else:
            print("‚ùå No vector store found. Run --setup first.")
        return

    if args.check_download:
        assistant.check_download_status("./downloaded_wiki/AKS")
        return
    
    if args.rebuild_progress:
        assistant.rebuild_progress_from_existing_files()
        return

    # Handle questions
    if args.ask:
        assistant.ask_question(args.ask)
    elif args.interactive:
        assistant.interactive_mode()
    elif not args.setup:
        print("‚ÑπÔ∏è  No action specified. Use --help for options.")
    

if __name__ == "__main__":
    main()