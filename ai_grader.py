import os
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from openai import AzureOpenAI
import uuid
import random

class AIResponseGrader:
    def __init__(self):
        """Initialize the AI Response Grader with Azure OpenAI client"""
        self.client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version="2025-04-01-preview",
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        self.deployment_name = os.environ.get("AZURE_OPENAI_MODEL_GRADER")
        self.evaluation_history = []
        
    def grade_responses(self, 
                       question: str, 
                       response_a: str, 
                       response_b: str,
                       context: str = "",
                       save_evaluation: bool = True) -> Dict:
        """
        Grade two responses without knowing which is AI vs human generated
        
        Args:
            question: The original question/issue
            response_a: First response (randomly assigned)
            response_b: Second response (randomly assigned)
            context: Additional context about the question
            save_evaluation: Whether to save the evaluation to history
            
        Returns:
            Dict containing detailed evaluation results
        """
        
        # Create evaluation ID
        eval_id = str(uuid.uuid4())[:8]
        timestamp = datetime.now().isoformat()
        
        print(f"üéØ Starting response evaluation [{eval_id}]...")
        
        # Prepare the grading prompt
        grading_prompt = f"""
You are an expert technical evaluator tasked with comparing two responses to a customer support question about Azure Kubernetes Service (AKS). 

**IMPORTANT:** You are evaluating these responses blindly - you do not know which one was generated by AI vs human. Please evaluate purely on merit.

**Original Question/Issue:**
{question}

**Additional Context:**
{context if context else "No additional context provided"}

**Response A:**
{response_a}

**Response B:**
{response_b}

**Evaluation Criteria:**
Please evaluate both responses across these dimensions (score 1-10 for each):

1. **Technical Accuracy**: How technically correct and precise is the information?
2. **Completeness**: How thoroughly does it address all aspects of the question?
3. **Clarity**: How clear and easy to understand is the explanation?
4. **Practical Value**: How actionable and useful is the response for the customer?
5. **Professional Tone**: How professional and appropriate is the communication style?
6. **Evidence/Citations**: How well does it provide supporting evidence or references?

**Please provide your evaluation in the following JSON format:**

```json
{{
  "overall_winner": "A" or "B",
  "overall_reasoning": "Brief explanation of why this response is better overall",
  "scores": {{
    "response_a": {{
      "technical_accuracy": score,
      "completeness": score,
      "clarity": score,
      "practical_value": score,
      "professional_tone": score,
      "evidence_citations": score,
      "total": sum_of_scores
    }},
    "response_b": {{
      "technical_accuracy": score,
      "completeness": score,
      "clarity": score,
      "practical_value": score,
      "professional_tone": score,
      "evidence_citations": score,
      "total": sum_of_scores
    }}
  }},
  "detailed_analysis": {{
    "response_a_strengths": ["list", "of", "strengths"],
    "response_a_weaknesses": ["list", "of", "weaknesses"],
    "response_b_strengths": ["list", "of", "strengths"],
    "response_b_weaknesses": ["list", "of", "weaknesses"]
  }},
  "specific_feedback": {{
    "response_a": "Detailed feedback for Response A",
    "response_b": "Detailed feedback for Response B"
  }}
}}
```

Please be thorough and objective in your evaluation. Focus on the quality of the technical content, completeness of the answer, and overall helpfulness to the customer.
"""

        try:
            # Get evaluation from AI grader
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are an expert technical evaluator specializing in Azure and cloud services. You evaluate responses objectively based on technical merit, completeness, and helpfulness."},
                    {"role": "user", "content": grading_prompt}
                ],
                temperature=0.1,  # Low temperature for consistent evaluation
                max_tokens=2000
            )
            
            evaluation_text = response.choices[0].message.content
            
            # Parse the JSON response
            try:
                # Extract JSON from the response
                json_start = evaluation_text.find('{')
                json_end = evaluation_text.rfind('}') + 1
                json_str = evaluation_text[json_start:json_end]
                evaluation_data = json.loads(json_str)
                
                # Add metadata
                evaluation_result = {
                    "evaluation_id": eval_id,
                    "timestamp": timestamp,
                    "question": question,
                    "context": context,
                    "responses": {
                        "response_a": response_a,
                        "response_b": response_b
                    },
                    "evaluation": evaluation_data,
                    "raw_evaluation_text": evaluation_text
                }
                
                if save_evaluation:
                    self.save_evaluation(evaluation_result)
                
                print(f"‚úÖ Evaluation completed [{eval_id}]")
                return evaluation_result
                
            except json.JSONDecodeError as e:
                print(f"‚ùå Error parsing evaluation JSON: {e}")
                print(f"Raw response: {evaluation_text}")
                return {
                    "evaluation_id": eval_id,
                    "timestamp": timestamp,
                    "error": "Failed to parse evaluation JSON",
                    "raw_response": evaluation_text
                }
                
        except Exception as e:
            print(f"‚ùå Error during evaluation: {e}")
            return {
                "evaluation_id": eval_id,
                "timestamp": timestamp,
                "error": str(e)
            }
    
    def save_evaluation(self, evaluation_result: Dict) -> None:
        """Save evaluation result to file"""
        try:
            # Create evaluations directory if it doesn't exist
            os.makedirs("evaluations", exist_ok=True)
            
            # Save individual evaluation
            eval_file = f"evaluations/evaluation_{evaluation_result['evaluation_id']}.json"
            with open(eval_file, 'w', encoding='utf-8') as f:
                json.dump(evaluation_result, f, indent=2, ensure_ascii=False)
            
            # Append to history
            self.evaluation_history.append(evaluation_result)
            
            # Save consolidated history
            history_file = "evaluations/evaluation_history.json"
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(self.evaluation_history, f, indent=2, ensure_ascii=False)
                
            print(f"üíæ Evaluation saved to {eval_file}")
            
        except Exception as e:
            print(f"‚ùå Error saving evaluation: {e}")
    
    def load_evaluation_history(self) -> List[Dict]:
        """Load evaluation history from file"""
        try:
            history_file = "evaluations/evaluation_history.json"
            if os.path.exists(history_file):
                with open(history_file, 'r', encoding='utf-8') as f:
                    self.evaluation_history = json.load(f)
                    print(f"‚úÖ Loaded {len(self.evaluation_history)} evaluations from history")
                    return self.evaluation_history
            return []
        except Exception as e:
            print(f"‚ùå Error loading evaluation history: {e}")
            return []
    
    def print_evaluation_summary(self, evaluation_result: Dict) -> None:
        """Print a formatted summary of the evaluation"""
        if "error" in evaluation_result:
            print(f"‚ùå Evaluation failed: {evaluation_result['error']}")
            return
        
        eval_data = evaluation_result["evaluation"]
        
        print(f"\nüìä Evaluation Summary [{evaluation_result['evaluation_id']}]")
        print("="*60)
        
        # Overall winner
        winner = eval_data["overall_winner"]
        print(f"üèÜ Overall Winner: Response {winner}")
        print(f"üí≠ Reasoning: {eval_data['overall_reasoning']}")
        
        # Scores
        print(f"\nüìà Detailed Scores:")
        print(f"Response A Total: {eval_data['scores']['response_a']['total']}/60")
        print(f"Response B Total: {eval_data['scores']['response_b']['total']}/60")
        
        print(f"\nüìã Score Breakdown:")
        criteria = ["technical_accuracy", "completeness", "clarity", "practical_value", "professional_tone", "evidence_citations"]
        for criterion in criteria:
            score_a = eval_data['scores']['response_a'][criterion]
            score_b = eval_data['scores']['response_b'][criterion]
            print(f"  {criterion.replace('_', ' ').title()}: A={score_a}, B={score_b}")
        
        # Strengths and weaknesses
        print(f"\n‚úÖ Response A Strengths:")
        for strength in eval_data['detailed_analysis']['response_a_strengths']:
            print(f"  ‚Ä¢ {strength}")
        
        print(f"\n‚ùå Response A Weaknesses:")
        for weakness in eval_data['detailed_analysis']['response_a_weaknesses']:
            print(f"  ‚Ä¢ {weakness}")
        
        print(f"\n‚úÖ Response B Strengths:")
        for strength in eval_data['detailed_analysis']['response_b_strengths']:
            print(f"  ‚Ä¢ {strength}")
        
        print(f"\n‚ùå Response B Weaknesses:")
        for weakness in eval_data['detailed_analysis']['response_b_weaknesses']:
            print(f"  ‚Ä¢ {weakness}")
        
        # Add the missing labels display when available
        if "response_labels" in evaluation_result:
            print(f"\nüè∑Ô∏è  Response Labels:")
            print(f"Response A: {evaluation_result['response_labels']['response_a']}")
            print(f"Response B: {evaluation_result['response_labels']['response_b']}")
            
            if "actual_winner" in evaluation_result:
                print(f"\nüéØ Actual Winner: {evaluation_result['actual_winner']}")
        
        print("="*60)
    
    def generate_comparison_report(self, evaluation_results: List[Dict]) -> str:
        """Generate a comprehensive comparison report from multiple evaluations"""
        if not evaluation_results:
            return "No evaluations to report on."
        
        valid_evaluations = [e for e in evaluation_results if "evaluation" in e]
        if not valid_evaluations:
            return "No valid evaluations found."
        
        # Count wins
        a_wins = sum(1 for e in valid_evaluations if e["evaluation"]["overall_winner"] == "A")
        b_wins = sum(1 for e in valid_evaluations if e["evaluation"]["overall_winner"] == "B")
        
        # Calculate average scores
        total_evals = len(valid_evaluations)
        avg_scores_a = {criterion: 0 for criterion in ["technical_accuracy", "completeness", "clarity", "practical_value", "professional_tone", "evidence_citations"]}
        avg_scores_b = {criterion: 0 for criterion in ["technical_accuracy", "completeness", "clarity", "practical_value", "professional_tone", "evidence_citations"]}
        
        for evaluation in valid_evaluations:
            scores = evaluation["evaluation"]["scores"]
            for criterion in avg_scores_a.keys():
                avg_scores_a[criterion] += scores["response_a"][criterion]
                avg_scores_b[criterion] += scores["response_b"][criterion]
        
        for criterion in avg_scores_a.keys():
            avg_scores_a[criterion] /= total_evals
            avg_scores_b[criterion] /= total_evals
        
        # Generate report
        report = f"""
üìä AI vs Human Response Comparison Report
{'='*50}

üìà Overall Results:
‚Ä¢ Total Evaluations: {total_evals}
‚Ä¢ Response A Wins: {a_wins} ({a_wins/total_evals*100:.1f}%)
‚Ä¢ Response B Wins: {b_wins} ({b_wins/total_evals*100:.1f}%)

üìã Average Scores (out of 10):
"""
        
        for criterion in avg_scores_a.keys():
            criterion_name = criterion.replace('_', ' ').title()
            report += f"‚Ä¢ {criterion_name}:\n"
            report += f"  - Response A: {avg_scores_a[criterion]:.1f}\n"
            report += f"  - Response B: {avg_scores_b[criterion]:.1f}\n"
        
        total_avg_a = sum(avg_scores_a.values())
        total_avg_b = sum(avg_scores_b.values())
        
        report += f"\nüéØ Total Average Scores:\n"
        report += f"‚Ä¢ Response A: {total_avg_a:.1f}/60\n"
        report += f"‚Ä¢ Response B: {total_avg_b:.1f}/60\n"
        
        return report


class AKSResponseTester:
    def __init__(self, aks_assistant, ai_grader):
        """Initialize the response tester with AKS assistant and AI grader"""
        self.aks_assistant = aks_assistant
        self.ai_grader = ai_grader
        
    def test_response_quality(self, 
                             question: str, 
                             human_response: str,
                             context: str = "",
                             label_responses: bool = False) -> Dict:
        """
        Test AI response quality against human response
        
        Args:
            question: The original question
            human_response: The human-generated response
            context: Additional context
            label_responses: Whether to reveal which is AI vs human (for debugging)
            
        Returns:
            Dict containing evaluation results with labels if requested
        """
        
        print(f"üß™ Testing response quality...")
        print(f"üìù Question: {question[:100]}...")
        
        # Generate AI response
        print("ü§ñ Generating AI response...")
        ai_response = self.generate_ai_response(question, context)
        
        if not ai_response:
            return {"error": "Failed to generate AI response"}
        
        # Randomly assign A/B to prevent bias
        random.seed()  # Ensure true randomization
        if random.choice([True, False]):
            response_a = ai_response
            response_b = human_response
            a_is_ai = True
        else:
            response_a = human_response
            response_b = ai_response
            a_is_ai = False

        # Debug print to verify randomization is working
        print(f"üîÄ Random assignment: A={'AI' if a_is_ai else 'Human'}, B={'Human' if a_is_ai else 'AI'}")
                
        # Grade the responses
        print("‚öñÔ∏è  Grading responses...")
        evaluation = self.ai_grader.grade_responses(
            question=question,
            response_a=response_a,
            response_b=response_b,
            context=context
        )
        
        # Add labeling if requested
        if label_responses:
            evaluation["response_labels"] = {
                "response_a": "AI" if a_is_ai else "Human",
                "response_b": "Human" if a_is_ai else "AI"
            }
            
            # Determine actual winner
            winner = evaluation["evaluation"]["overall_winner"]
            actual_winner = "AI" if (winner == "A" and a_is_ai) or (winner == "B" and not a_is_ai) else "Human"
            evaluation["actual_winner"] = actual_winner
        
        return evaluation
    
    def generate_ai_response(self, question: str, context: str = "") -> Optional[str]:
        """Generate AI response using the AKS assistant"""
        try:
            # Create a temporary thread for this question
            thread = self.aks_assistant.client.beta.threads.create(
                tool_resources={
                    "file_search": {
                        "vector_store_ids": [self.aks_assistant.vector_store_id]
                    }
                }
            )
            
            # Add the question with context
            full_question = f"{question}\n\nContext: {context}" if context else question
            
            self.aks_assistant.client.beta.threads.messages.create(
                thread_id=thread.id,
                role="user",
                content=full_question
            )
            
            # Run the assistant
            run = self.aks_assistant.client.beta.threads.runs.create_and_poll(
                thread_id=thread.id,
                assistant_id=self.aks_assistant.assistant_id,
                instructions="""You are an expert in AKS (Azure Kubernetes Service) support. 
                Provide a comprehensive, technically accurate response to this customer question.
                Use the documentation to provide specific guidance and actionable steps.
                Include relevant references and be professional in your tone.
                Format your response clearly with proper structure.""",
                tools=[{"type": "file_search"}]
            )
            
            if run.status == 'completed':
                # Get the latest assistant message
                messages = self.aks_assistant.client.beta.threads.messages.list(thread_id=thread.id)
                
                for message in messages:
                    if message.role == "assistant":
                        for content in message.content:
                            if hasattr(content, 'text'):
                                text_content = content.text.value
                                annotations = getattr(content.text, 'annotations', [])
                                
                                # Process citations
                                final_content = self.aks_assistant.process_citations(text_content, annotations)
                                return final_content
            
            return None
            
        except Exception as e:
            print(f"‚ùå Error generating AI response: {e}")
            return None


# Test cases for demonstration
def create_test_cases():
    """Create test cases for evaluation"""
    return [
        {
            "question": """We received a question from a customer regarding the following update in the AKS release notes:

"AKS nodes now use Azure Container Registry (ACR)-scoped Entra ID tokens for kubelet authentication when pulling images from ACR. This enhancement replaces the legacy ARM-based Entra token model, aligning with modern security practices by scoping credentials directly to the registry and improving isolation and traceability."

https://github.com/Azure/AKS/releases/tag/2025-05-19

The customer understands that this change enhances security by scoping tokens directly to ACR.
However, they would like clarification on the following points:
1. Are there any considerations or required actions when operating existing AKS clusters under this new model?
2. Is there any official documentation that explains this change in more detail?
3. Does this update mean that, instead of using ARM-scoped tokens, kubelet now obtains a token scoped specifically to ACR, and that this token includes the acrpull role assignment?""",
            
            "human_response": """Here is a detailed explanation you can pass to the customer.

Today, when an AKS node needs to pull an image, the node kubelet's "acr-credential-provider" plug-in first authenticates to Microsoft Entra. When it authenticates to Entra, it obtains an Entra token, after which it exchanges that Entra token with Azure Container Registry (ACR) for an ACR data plane access token. Historically the "acr-credential-provider" plug-in asked Entra for an Entra token whose audience claim was Azure Resource Manager (ARM). Although that worked, the Entra token could in theory be used against other ARM endpoints and other non-ACR Azure resources, and it was wider in scope than strictly necessary.

Starting with the 19 May 2025 AKS node-image release, the "acr-credential provider" instead requests an Entra token whose audience is ACR itself (https://containerregistry.azure.net/), so that the Entra token can only be exchanged for an ACR data plane access token. The rest of the flow is unchanged: the ACR-audience Entra token is exchanged for an ACR data plane access token, which containerd then uses to pull the layers. This tighter scoping improves least-privilege, auditability, and prepares clusters for ACR registries that reject ARM-audience tokens altogether.

Do customers need to do anything?

For most existing clusters, no immediate action is required. Nodes that have already upgraded to, or are created from, a node image dated 2025-05-19 (or later) automatically use the tighter-scoped ACR audience Entra token during image pull flows. Clusters that stay on older node images will continue to present overly-broad ARM-audience Entra tokens; these will keep working until the ACR registry owner decides to disable ARM-audience authentication. Therefore the recommended path is simply to upgrade node pools before turning on the ACR registry-side control.

Key reminder about roles:

Nothing in this change grants pull permissions to the cluster by itself. The managed identity (or service principal) used by the nodes must still hold an AcrPull role assignment on the target registry where the image is being pulled. Without that role, image pulls will fail regardless of Entra token audience.

Registry-side hardening to reject overly-broad ARM-audience Entra tokens (and only accept ACR-audience Entra tokens during authN):

ACR now exposes a setting called "Disable authentication as ARM". When you flip this config: either via the Azure portal or az acr update, the ACR registry will refuse any ARM-audience Entra token and accept only ACR-audience tokens. Clusters that have picked up the new node image will continue to work seamlessly once this registry hardening config is flipped. Older nodes still sending ARM-audience tokens will see authentication failures, which is why an upgrade is advisable before registry-side hardening enforcement.

Recommended next steps for the customer:

1. Check each node pool's image version; upgrade pools that pre-date 2025-05-19.
2. Confirm the node identity (managed or user-assigned) has an AcrPull role on the registry.
3. When ready, enable "Disable authentication as ARM" on the registry (or apply the matching Azure Policy) to complete the hardening.

Feel free to forward this explanation directly, and let me know if any further detail would help.""",
            
            "context": "Customer support question about AKS security enhancement regarding ACR token scoping"
        }
    ]


def main():
    """Main function for testing the AI grader"""
    
    # Check environment variables
    if not os.getenv("AZURE_OPENAI_API_KEY") or not os.getenv("AZURE_OPENAI_ENDPOINT"):
        print("‚ùå Please set AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT environment variables")
        return
    
    # Initialize components
    print("üöÄ Initializing AI Response Grader...")
    ai_grader = AIResponseGrader()
    
    # Load test cases
    test_cases = create_test_cases()
    
    print(f"üìù Found {len(test_cases)} test cases")
    
    # For now, let's just test the grader with the example
    if test_cases:
        test_case = test_cases[0]
        
        print("\nüß™ Testing AI grader with example case...")
        
        # Create a mock AI response for testing
        mock_ai_response = """Thank you for your question about the AKS security enhancement regarding ACR token scoping.

**Understanding the Change:**
This update modifies how AKS nodes authenticate when pulling images from Azure Container Registry (ACR). Previously, nodes used ARM-scoped Entra ID tokens, but now they use ACR-scoped tokens for better security isolation.

**Key Points:**

1. **No immediate action required for most clusters** - The change is automatic when nodes upgrade to the May 19, 2025 image or later.

2. **Improved security** - ACR-scoped tokens can only be used with ACR, reducing the potential attack surface compared to ARM-scoped tokens.

3. **Role assignments still required** - Your cluster's managed identity must still have the AcrPull role on the target ACR registry.

**Recommended Actions:**
- Upgrade node pools to images dated 2025-05-19 or later
- Verify AcrPull role assignments are in place
- Consider enabling "Disable authentication as ARM" on your ACR registries for additional hardening

**Documentation:**
You can find more details about ACR authentication and AKS integration in the official Azure documentation for Container Registry and AKS security best practices.

This change enhances security by implementing the principle of least privilege while maintaining backward compatibility during the transition period."""
        
        # Test the grader
        evaluation = ai_grader.grade_responses(
            question=test_case["question"],
            response_a=test_case["human_response"],
            response_b=mock_ai_response,
            context=test_case["context"]
        )
        
        # Print results
        ai_grader.print_evaluation_summary(evaluation)
        
        # Save to demonstrate the system
        print(f"\nüíæ Evaluation saved to evaluations/ directory")
        
        print("\nüéâ AI Grader test completed successfully!")
        print("You can now integrate this with your AKS assistant for full testing.")

if __name__ == "__main__":
    main()